{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nfrom thundersvm import SVC as svmgpu\\nimport calculateWeightUsingGa2 as aresult\\nimport pandas as pd\\nimport itertools\\nimport random\\nfrom sklearn.ensemble import VotingClassifier\\n\\nrandomseed = 42\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nfrom thundersvm import SVC as svmgpu\\nimport calculateWeightUsingGa2 as aresult\\nimport pandas as pd\\nimport itertools\\nimport random\\nfrom sklearn.ensemble import VotingClassifier\\n\\nrandomseed = 42\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics as m\n",
    "from thundersvm import SVC as svmgpu\n",
    "import calculateWeightUsingGa2 as aresult\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "randomseed = 42\n",
    "np.random.seed(randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Dataset \n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"xtest = np.array(pd.read_csv(\\\"../dataset/xtest.txt\\\"))\\nxtrain = np.array(pd.read_csv(\\\"../dataset/xtrain.txt\\\"))\\nytest_original = np.array(pd.read_csv(\\\"../dataset/ytest.txt\\\")).ravel()\\nytrain_original = np.array(pd.read_csv(\\\"../dataset/ytrain.txt\\\")).ravel()\\n\\nytrain = ytrain_original.copy()\\nytest = ytest_original.copy()\\n\\n\\nreal_xtest=xtest\\nreal_ytest=ytest_original\\n\\nx=xtrain\\ny=ytrain_original\";\n",
       "                var nbb_formatted_code = \"xtest = np.array(pd.read_csv(\\\"../dataset/xtest.txt\\\"))\\nxtrain = np.array(pd.read_csv(\\\"../dataset/xtrain.txt\\\"))\\nytest_original = np.array(pd.read_csv(\\\"../dataset/ytest.txt\\\")).ravel()\\nytrain_original = np.array(pd.read_csv(\\\"../dataset/ytrain.txt\\\")).ravel()\\n\\nytrain = ytrain_original.copy()\\nytest = ytest_original.copy()\\n\\n\\nreal_xtest = xtest\\nreal_ytest = ytest_original\\n\\nx = xtrain\\ny = ytrain_original\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtest = np.array(pd.read_csv(\"../dataset/xtest.txt\"))\n",
    "xtrain = np.array(pd.read_csv(\"../dataset/xtrain.txt\"))\n",
    "ytest_original = np.array(pd.read_csv(\"../dataset/ytest.txt\")).ravel()\n",
    "ytrain_original = np.array(pd.read_csv(\"../dataset/ytrain.txt\")).ravel()\n",
    "\n",
    "ytrain = ytrain_original.copy()\n",
    "ytest = ytest_original.copy()\n",
    "\n",
    "\n",
    "real_xtest=xtest\n",
    "real_ytest=ytest_original\n",
    "\n",
    "x=xtrain\n",
    "y=ytrain_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=42, shuffle=True)\n",
      "************************** ==>  0\n",
      "original score 0.6799804076573547\n",
      "RF f1_score 0.6799804076573547\n",
      "RF svc 0.6829414566477141\n",
      "RF xgbc 0.7421233006475806\n",
      "[22, 110, 397, 29, 119, 374, 337, 475, 752, 230]\n",
      "0  ====================  [0 1 2 3 5 6 8]\n",
      "0.7080005143220071\n",
      "0.7332794584683437\n",
      "1  ====================  [ 0  1  2  5  9 10 11]\n",
      "0.7170745526311394\n",
      "0.7332794584683437\n",
      "2  ====================  [ 0  3  4  5  9 10 11]\n",
      "0.6719766362066047\n",
      "0.7332794584683437\n",
      "3  ====================  [ 0  1  2  3  5  7 11]\n",
      "0.6958001063264222\n",
      "0.7332794584683437\n",
      "4  ====================  [ 0  1  2  6  8 10 11]\n",
      "0.7126213828061773\n",
      "0.7332794584683437\n",
      "5  ====================  [ 0  2  6  7  8 10 11]\n",
      "0.6916303911486211\n",
      "0.7332794584683437\n",
      "6  ====================  [ 0  2  4  5  7 10 11]\n",
      "0.6822086819264511\n",
      "0.7332794584683437\n",
      "7  ====================  [ 1  2  3  4  5  9 11]\n",
      "0.6460477263271805\n",
      "0.7332794584683437\n",
      "8  ====================  [ 2  5  6  7  9 10 11]\n",
      "0.4062748932323086\n",
      "0.7332794584683437\n",
      "9  ====================  [ 0  1  4  8  9 10 11]\n",
      "0.7072393704710865\n",
      "0.7332794584683437\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.55251157628907 [0.5427746  0.61712943 0.99993908 0.69783387 0.81866204 0.73326032\n",
      " 0.81961176 0.48927796 0.8214749  0.65288586 0.84750896 0.70321769\n",
      " 0.85399242 0.61962656 0.8597681  0.59585479 0.90136916 0.25471508\n",
      " 0.91165168 0.19699089 0.94738722 0.69651733 0.99192473]\n",
      "f1_score 0.7387264789029022\n",
      "accuracy_score 0.7366666666666667\n",
      "************************** ==>  1\n",
      "original score 0.6747002805365053\n",
      "RF f1_score 0.6747002805365053\n",
      "RF svc 0.6867901462221948\n",
      "RF xgbc 0.7412993237360219\n",
      "[308, 104, 367, 575, 614, 192, 372, 604, 401, 138]\n",
      "0  ====================  [ 0  2  3  6  7  8 10]\n",
      "0.6827948539963293\n",
      "0.7441810057356276\n",
      "1  ====================  [ 0  1  2  5  7  9 10]\n",
      "0.7045863500655773\n",
      "0.7441810057356276\n",
      "2  ====================  [ 0  2  5  7  8  9 10]\n",
      "0.6771153334073269\n",
      "0.7441810057356276\n",
      "3  ====================  [ 1  2  5  6  8 10 11]\n",
      "0.6428825386851247\n",
      "0.7441810057356276\n",
      "4  ====================  [ 1  3  4  6  8  9 10]\n",
      "0.658113415232213\n",
      "0.7441810057356276\n",
      "5  ====================  [ 0  1  3  7  8  9 11]\n",
      "0.7029394995248653\n",
      "0.7441810057356276\n",
      "6  ====================  [ 0  2  6  7  8  9 10]\n",
      "0.6575084392731452\n",
      "0.7441810057356276\n",
      "7  ====================  [ 1  3  4  5  8  9 10]\n",
      "0.653879271036495\n",
      "0.7441810057356276\n",
      "8  ====================  [ 0  3  4  6  7  9 10]\n",
      "0.6914931689493095\n",
      "0.7441810057356276\n",
      "9  ====================  [ 0  1  3  4  5  9 10]\n",
      "0.6961220608862942\n",
      "0.7441810057356276\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.882410113532002 [0.4854553  0.68367419 0.82507941 0.57605987 0.83852513 0.8014364\n",
      " 0.86658205 0.48597195 0.86675421 0.23019001 0.8727873  0.45008629\n",
      " 0.873083   0.79497223 0.94771313 0.38566504 0.95771716 0.32777696\n",
      " 0.9640512  0.74311276 0.97920487 0.77787339 0.98916252]\n",
      "f1_score 0.7430972592262914\n",
      "accuracy_score 0.7416666666666667\n",
      "************************** ==>  2\n",
      "original score 0.6773429356581067\n",
      "RF f1_score 0.6773429356581067\n",
      "RF svc 0.6849694009271513\n",
      "RF xgbc 0.7393072272017802\n",
      "[59, 100, 605, 273, 438, 395, 435, 230, 185, 33]\n",
      "0  ====================  [ 0  1  2  4  5  6 10]\n",
      "0.7132165009444296\n",
      "0.7362259062194018\n",
      "1  ====================  [ 0  1  2  5  6 10 11]\n",
      "0.7349669148056245\n",
      "0.7362259062194018\n",
      "2  ====================  [ 1  3  4  5  8  9 11]\n",
      "0.6463724265960803\n",
      "0.7362259062194018\n",
      "3  ====================  [ 0  2  3  4  6  8 11]\n",
      "0.6976995082718181\n",
      "0.7362259062194018\n",
      "4  ====================  [ 0  4  5  6  7  9 11]\n",
      "0.6696598559343658\n",
      "0.7362259062194018\n",
      "5  ====================  [ 0  3  4  5  8  9 11]\n",
      "0.6856056239582373\n",
      "0.7362259062194018\n",
      "6  ====================  [ 0  4  5  6  7  8 10]\n",
      "0.6715721912683673\n",
      "0.7362259062194018\n",
      "7  ====================  [ 0  1  4  8  9 10 11]\n",
      "0.7051728246595155\n",
      "0.7362259062194018\n",
      "8  ====================  [ 0  1  3  6  7  9 11]\n",
      "0.7083280550702648\n",
      "0.7362259062194018\n",
      "9  ====================  [ 0  1  2  3  5  9 10]\n",
      "0.7228487568211385\n",
      "0.7362259062194018\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.87607052043425 [0.42240855 0.48548501 0.98427575 0.70905999 0.854146   0.82382139\n",
      " 0.85796552 0.22778527 0.85889095 0.58678181 0.86776629 0.30229535\n",
      " 0.91183948 0.53683918 0.92254971 0.41972587 0.92339649 0.59767858\n",
      " 0.94846791 0.65856542 0.94944299 0.78198697 0.96355799]\n",
      "f1_score 0.7365306974047158\n",
      "accuracy_score 0.735\n",
      "************************** ==>  3\n",
      "original score 0.691018775833022\n",
      "RF f1_score 0.691018775833022\n",
      "RF svc 0.6812095941113174\n",
      "RF xgbc 0.7294122428969135\n",
      "[704, 461, 72, 619, 253, 703, 247, 614, 674, 112]\n",
      "0  ====================  [ 2  3  4  7  8 10 11]\n",
      "0.4373454311479304\n",
      "0.7319685213465631\n",
      "1  ====================  [ 0  6  7  8  9 10 11]\n",
      "0.6484176742797433\n",
      "0.7319685213465631\n",
      "2  ====================  [0 1 2 4 6 7 9]\n",
      "0.7065545464597951\n",
      "0.7319685213465631\n",
      "3  ====================  [ 1  3  4  7  8  9 11]\n",
      "0.6546318650442502\n",
      "0.7319685213465631\n",
      "4  ====================  [0 2 3 4 5 6 8]\n",
      "0.6783522376774797\n",
      "0.7319685213465631\n",
      "5  ====================  [ 2  3  4  7  8  9 11]\n",
      "0.4694550191205176\n",
      "0.7319685213465631\n",
      "6  ====================  [ 0  1  6  7  8  9 11]\n",
      "0.6688535685745893\n",
      "0.7319685213465631\n",
      "7  ====================  [ 1  3  4  6  8  9 10]\n",
      "0.6623434715877856\n",
      "0.7319685213465631\n",
      "8  ====================  [ 2  3  4  5  6  7 10]\n",
      "0.47140659513780947\n",
      "0.7319685213465631\n",
      "9  ====================  [ 0  1  2  6  7  8 10]\n",
      "0.7323813383609452\n",
      "0.7319685213465631\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.464418196580763 [0.67523242 0.67114085 0.82436737 0.32284367 0.82509701 0.50808001\n",
      " 0.87896692 0.69074784 0.89244262 0.51487869 0.91743516 0.6526444\n",
      " 0.91841769 0.39021578 0.92179424 0.52646679 0.93344622 0.51666519\n",
      " 0.94813724 0.49299263 0.95997897 0.98926286 0.97488701]\n",
      "f1_score 0.7316742756051469\n",
      "accuracy_score 0.73\n",
      "************************** ==>  4\n",
      "original score 0.6765297679740189\n",
      "RF f1_score 0.6765297679740189\n",
      "RF svc 0.6832358226829387\n",
      "RF xgbc 0.7443720011582003\n",
      "[46, 243, 43, 759, 213, 229, 393, 671, 691, 604]\n",
      "0  ====================  [0 1 2 3 7 8 9]\n",
      "0.7160217905260399\n",
      "0.7350107006603003\n",
      "1  ====================  [ 0  1  5  7  8 10 11]\n",
      "0.7127988336578005\n",
      "0.7350107006603003\n",
      "2  ====================  [ 0  1  2  3  6  9 10]\n",
      "0.7164206223673543\n",
      "0.7350107006603003\n",
      "3  ====================  [ 3  4  5  6  7  9 10]\n",
      "0.43730874747263565\n",
      "0.7350107006603003\n",
      "4  ====================  [ 0  1  4  5  8  9 11]\n",
      "0.7084981066972809\n",
      "0.7350107006603003\n",
      "5  ====================  [ 0  1  4  7  9 10 11]\n",
      "0.7145236515617367\n",
      "0.7350107006603003\n",
      "6  ====================  [ 0  3  4  5  7 10 11]\n",
      "0.6702919222330591\n",
      "0.7350107006603003\n",
      "7  ====================  [ 1  6  7  8  9 10 11]\n",
      "0.6060609104331532\n",
      "0.7350107006603003\n",
      "8  ====================  [ 2  3  4  5  9 10 11]\n",
      "0.4294450571332292\n",
      "0.7350107006603003\n",
      "9  ====================  [ 1  3  4  5  8  9 10]\n",
      "0.6779793534370863\n",
      "0.7350107006603003\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.60228818749356 [0.46849658 0.65479027 0.99823362 0.74653029 0.84129122 0.71153587\n",
      " 0.84188431 0.74752147 0.87465599 0.36875527 0.8948434  0.65718299\n",
      " 0.94221762 0.71434384 0.95545164 0.41680661 0.95766879 0.3887533\n",
      " 0.95982225 0.2555924  0.96481634 0.60478813 0.96643233]\n",
      "f1_score 0.7361281737002059\n",
      "accuracy_score 0.735\n",
      "************************** ==>  5\n",
      "original score 0.6671609073893565\n",
      "RF f1_score 0.6671609073893565\n",
      "RF svc 0.6815015718384989\n",
      "RF xgbc 0.7339594024620291\n",
      "[245, 499, 54, 634, 355, 612, 329, 96, 546, 157]\n",
      "0  ====================  [ 0  1  5  8  9 10 11]\n",
      "0.7137741504493931\n",
      "0.7404095045893115\n",
      "1  ====================  [ 1  2  3  5  6  7 10]\n",
      "0.664438434690151\n",
      "0.7404095045893115\n",
      "2  ====================  [ 0  1  2  3  8 10 11]\n",
      "0.7014374514374515\n",
      "0.7404095045893115\n",
      "3  ====================  [ 1  3  5  7  8  9 11]\n",
      "0.657684594988858\n",
      "0.7404095045893115\n",
      "4  ====================  [ 0  2  4  7  9 10 11]\n",
      "0.6728494676456652\n",
      "0.7404095045893115\n",
      "5  ====================  [ 1  3  4  6  7  9 11]\n",
      "0.654155600187569\n",
      "0.7404095045893115\n",
      "6  ====================  [ 0  2  4  5  6  9 10]\n",
      "0.6846729359844016\n",
      "0.7404095045893115\n",
      "7  ====================  [ 0  1  2  5  6  8 10]\n",
      "0.7076309189267013\n",
      "0.7404095045893115\n",
      "8  ====================  [ 1  2  4  5  7  9 11]\n",
      "0.6667129965395637\n",
      "0.7404095045893115\n",
      "9  ====================  [ 0  1  3  4  8  9 10]\n",
      "0.6870956269845673\n",
      "0.7404095045893115\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.841814443930398 [0.48361588 0.59517563 0.77993191 0.77579395 0.79633717 0.39433084\n",
      " 0.85247453 0.72497518 0.87291539 0.36358396 0.92015502 0.58563376\n",
      " 0.94960953 0.33673058 0.9518641  0.59517699 0.96447203 0.73991796\n",
      " 0.96572499 0.41539938 0.98233474 0.64664651 0.99187985]\n",
      "f1_score 0.7377238581198398\n",
      "accuracy_score 0.7366666666666667\n",
      "************************** ==>  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original score 0.6857235886775732\n",
      "RF f1_score 0.6857235886775732\n",
      "RF svc 0.6832358226829387\n",
      "RF xgbc 0.7318229892934472\n",
      "[609, 240, 783, 586, 697, 10, 516, 84, 162, 417]\n",
      "0  ====================  [ 1  3  4  6  7  8 10]\n",
      "0.6540129502997267\n",
      "0.7417458338125925\n",
      "1  ====================  [ 0  1  5  6  9 10 11]\n",
      "0.7074783342067502\n",
      "0.7417458338125925\n",
      "2  ====================  [ 3  6  7  8  9 10 11]\n",
      "0.40253257909881535\n",
      "0.7417458338125925\n",
      "3  ====================  [ 1  2  6  8  9 10 11]\n",
      "0.6602944148676892\n",
      "0.7417458338125925\n",
      "4  ====================  [ 2  3  4  6  7 10 11]\n",
      "0.44898245757309724\n",
      "0.7417458338125925\n",
      "5  ====================  [ 0  1  2  3  4  6 11]\n",
      "0.7156699154950844\n",
      "0.7417458338125925\n",
      "6  ====================  [ 1  2  3  5  9 10 11]\n",
      "0.6544433801899312\n",
      "0.7417458338125925\n",
      "7  ====================  [ 0  1  2  4  7  9 10]\n",
      "0.7111419046774569\n",
      "0.7417458338125925\n",
      "8  ====================  [0 1 3 5 6 7 9]\n",
      "0.6858035862373448\n",
      "0.7417458338125925\n",
      "9  ====================  [ 0  3  5  6  7  9 11]\n",
      "0.6663964227944967\n",
      "0.7417458338125925\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.542199663344178 [0.62595834 0.61970343 0.79223699 0.40195283 0.81140043 0.69120746\n",
      " 0.81734355 0.36085574 0.85843589 0.55813749 0.87753326 0.36219053\n",
      " 0.88457132 0.78716881 0.92061183 0.51943293 0.94127645 0.73636778\n",
      " 0.97909483 0.64554095 0.97936813 0.56700012 0.98169259]\n",
      "f1_score 0.7383944731647208\n",
      "accuracy_score 0.7366666666666667\n",
      "************************** ==>  7\n",
      "original score 0.6774642799721772\n",
      "RF f1_score 0.6774642799721772\n",
      "RF svc 0.6849694009271513\n",
      "RF xgbc 0.7365104138658676\n",
      "[705, 84, 64, 540, 250, 482, 60, 137, 344, 577]\n",
      "0  ====================  [ 2  3  4  7  9 10 11]\n",
      "0.43996693241694573\n",
      "0.7419444638059267\n",
      "1  ====================  [ 0  1  2  4  7  9 10]\n",
      "0.6803030536457628\n",
      "0.7419444638059267\n",
      "2  ====================  [ 0  1  2  4  5  7 11]\n",
      "0.6924631665082263\n",
      "0.7419444638059267\n",
      "3  ====================  [ 1  2  4  5  6  9 11]\n",
      "0.6368519710866173\n",
      "0.7419444638059267\n",
      "4  ====================  [ 0  1  6  8  9 10 11]\n",
      "0.6998165653338068\n",
      "0.7419444638059267\n",
      "5  ====================  [ 1  2  3  4  6  8 10]\n",
      "0.6489309825672026\n",
      "0.7419444638059267\n",
      "6  ====================  [ 0  1  2  4  5  6 11]\n",
      "0.6949174367133795\n",
      "0.7419444638059267\n",
      "7  ====================  [ 0  1  3  4  5  8 11]\n",
      "0.7209128573296939\n",
      "0.7419444638059267\n",
      "8  ====================  [ 0  2  4  6  7  8 11]\n",
      "0.6781470163823105\n",
      "0.7419444638059267\n",
      "9  ====================  [ 1  2  5  7  8  9 10]\n",
      "0.6633674881329508\n",
      "0.7419444638059267\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.827469832106917 [0.50221169 0.677873   0.85784519 0.18894551 0.86601429 0.59387215\n",
      " 0.89287339 0.6861994  0.90312582 0.20799103 0.91030874 0.74851322\n",
      " 0.92464655 0.41491443 0.92797348 0.69543543 0.97463843 0.80343345\n",
      " 0.98507927 0.56090769 0.99503707 0.46781277 0.99850419]\n",
      "f1_score 0.734564900309908\n",
      "accuracy_score 0.7333333333333333\n",
      "************************** ==>  8\n",
      "original score 0.6470741747154887\n",
      "RF f1_score 0.6470741747154887\n",
      "RF svc 0.6846726042921341\n",
      "RF xgbc 0.7296399135120212\n",
      "[403, 549, 789, 738, 737, 188, 370, 20, 196, 470]\n",
      "0  ====================  [ 0  3  4  6  7 10 11]\n",
      "0.6891374931851311\n",
      "0.7388527423345653\n",
      "1  ====================  [ 1  2  4  5  8  9 11]\n",
      "0.6640061884471394\n",
      "0.7388527423345653\n",
      "2  ====================  [ 4  5  7  8  9 10 11]\n",
      "0.39757058359367403\n",
      "0.7388527423345653\n",
      "3  ====================  [ 2  4  5  7  8  9 10]\n",
      "0.42107769345230345\n",
      "0.7388527423345653\n",
      "4  ====================  [ 2  4  5  6  9 10 11]\n",
      "0.4161063376049028\n",
      "0.7388527423345653\n",
      "5  ====================  [ 0  1  3  6  8  9 11]\n",
      "0.6996416656098493\n",
      "0.7388527423345653\n",
      "6  ====================  [ 0  2  5  7  9 10 11]\n",
      "0.6809291295326424\n",
      "0.7388527423345653\n",
      "7  ====================  [ 0  1  2  3  4 10 11]\n",
      "0.70289423633507\n",
      "0.7388527423345653\n",
      "8  ====================  [0 1 4 5 6 7 8]\n",
      "0.6978770457674117\n",
      "0.7388527423345653\n",
      "9  ====================  [ 1  2  3  4  5  7 11]\n",
      "0.6488722765318509\n",
      "0.7388527423345653\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.225014899700142 [0.37926692 0.70599347 0.81310773 0.76621971 0.83272596 0.48444165\n",
      " 0.84116838 0.12431399 0.8591308  0.36980641 0.89460845 0.29858277\n",
      " 0.93224016 0.79732592 0.93646708 0.64434357 0.97159464 0.81065981\n",
      " 0.97492367 0.79368273 0.97690823 0.4554546  0.99741424]\n",
      "f1_score 0.735827372599621\n",
      "accuracy_score 0.735\n",
      "************************** ==>  9\n",
      "original score 0.7096216001193376\n",
      "RF f1_score 0.7096216001193376\n",
      "RF svc 0.6829414566477141\n",
      "RF xgbc 0.7250970501545239\n",
      "[751, 123, 578, 93, 247, 775, 157, 58, 627, 223]\n",
      "0  ====================  [ 2  5  6  7  8 10 11]\n",
      "0.401779671092548\n",
      "0.7312220794550779\n",
      "1  ====================  [ 0  1  2  7  8 10 11]\n",
      "0.7217772804107981\n",
      "0.7312220794550779\n",
      "2  ====================  [ 1  2  5  7  8  9 11]\n",
      "0.6595815669800402\n",
      "0.7312220794550779\n",
      "3  ====================  [ 0  1  2  5  6  7 10]\n",
      "0.7245299049402686\n",
      "0.7312220794550779\n",
      "4  ====================  [ 0  1  6  7  8  9 11]\n",
      "0.6894744974613684\n",
      "0.7312220794550779\n",
      "5  ====================  [ 3  4  6  8  9 10 11]\n",
      "0.4425834233279676\n",
      "0.7312220794550779\n",
      "6  ====================  [ 0  1  3  4  8  9 10]\n",
      "0.6968685130383063\n",
      "0.7312220794550779\n",
      "7  ====================  [0 1 2 4 5 6 9]\n",
      "0.7132673609792929\n",
      "0.7312220794550779\n",
      "8  ====================  [ 1  3  5  6  7  9 11]\n",
      "0.66208526524145\n",
      "0.7312220794550779\n",
      "9  ====================  [ 0  1  4  6  8  9 11]\n",
      "0.7012601913712534\n",
      "0.7312220794550779\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.585814376304349 [0.7466038  0.4801632  0.78999703 0.28871968 0.80324909 0.77872295\n",
      " 0.82288983 0.39662849 0.82767986 0.77951684 0.83135855 0.57492844\n",
      " 0.91495795 0.32908175 0.96457836 0.5882785  0.96671929 0.74830337\n",
      " 0.96895242 0.41881534 0.96933646 0.71886251 0.99713359]\n",
      "f1_score 0.7425681559855807\n",
      "accuracy_score 0.7416666666666667\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"original_scores = []\\ntrial1_scores = [] \\n\\nfrom sklearn.model_selection import KFold\\n\\nkf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\\nkf.get_n_splits(x)\\n\\nprint(kf)\\n\\ndata_cv = []\\n\\nfor train_index, test_index in kf.split(x):\\n    # print(\\\"TRAIN:\\\", train_index, \\\"TEST:\\\", test_index)\\n    X_train, X_test = x[train_index], x[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    data_cv.append([[X_train, X_test], [y_train, y_test]])\\n\\n\\nfor i in range(len(data_cv)):\\n    print(\\\"************************** ==> \\\", i)\\n    \\n    xtrain = data_cv[i][0][0]\\n    ytrain_original = data_cv[i][1][0]    \\n    xtest = real_xtest  # data_cv[i][0][1]\\n    ytest_original = real_ytest  # data_cv[i][1][1]    \\n    \\n    ytrain=ytrain_original.copy()\\n    ytest=ytest_original.copy() \\n    \\n    # member values\\n    clf = []\\n    acc = []\\n    finalacc = []\\n    ypredproba_all = []\\n    ypredconfprob_all = []\\n    \\n    # orginal score using random forest classifier\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    print(\\\"original score\\\", m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    \\n    \\n    # generate three base classifers using RF,svm and XGBoost\\n\\n    #================================================= \\n\\n    rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain,ytrain)\\n    rfpred=rf.predict(xtest)\\n    print('RF f1_score',m.f1_score(ytest,rfpred,average='weighted'))\\n\\n    clf.append(rf)\\n    acc.append(m.f1_score(ytest,rfpred,average='weighted'))\\n    ypredproba_all.append(rf.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,rfpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    #=================================================\\n    svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\\n    svc.fit(xtrain,ytrain)\\n\\n    svcpred=svc.predict(xtest)\\n    print('RF svc',m.f1_score(ytest,svcpred,average='weighted'))\\n\\n    clf.append(svc)\\n    acc.append(m.f1_score(ytest,svcpred,average='weighted'))\\n    ypredproba_all.append(svc.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,svcpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    #=================================================\\n    xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\\n    xgbc.fit(xtrain,ytrain)\\n\\n    xgbpred=xgbc.predict(xtest)\\n    print('RF xgbc',m.f1_score(ytest,xgbpred,average='weighted'))\\n\\n\\n    clf.append(xgbc)\\n    acc.append(m.f1_score(ytest,xgbpred,average='weighted'))\\n    ypredproba_all.append(xgbc.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,xgbpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    # =================================================\\n    # =================================================\\n    # generate combinations of features 12,7\\n    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\\n\\n    # generate 10 random numbers\\n    randnums = []\\n    for i in range(10):\\n        randnums.append(random.randrange(0, len(comb)))\\n\\n    print(randnums)\\n\\n    comb = np.array(comb)[randnums, :]\\n\\n\\n    for i in range(len(comb)):\\n        print(i, \\\" ==================== \\\", comb[i])\\n\\n        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\\n        rf.fit(xtrain[:, comb[i]], ytrain)\\n        rfpred = rf.predict(xtest[:, comb[i]])\\n        print(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n        clf.append(rf)\\n        acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\\n\\n        confmat = m.confusion_matrix(ytest, rfpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\\n        xgbmodel.fit(xtrain, ytrain)\\n        xgbmodelpred = xgbmodel.predict(xtest)\\n        print(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n\\n        clf.append(xgbmodel)\\n        acc.append(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(xgbmodel.predict_proba(xtest))\\n\\n        confmat = m.confusion_matrix(ytest, xgbmodelpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n    # #=================================================\\n    #Compute the weight using ga and compute the ensemble accuracy\\n    import calculateWeightUsingGa2 as aresult\\n    weightvalga=aresult.getbestvalues(acc)\\n\\n    finalval=0\\n    for i in range(len(acc)):\\n        finalval += weightvalga[i]*ypredproba_all[i]\\n\\n    print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\\n    print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\\n    trial1_scores.append(m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\";\n",
       "                var nbb_formatted_code = \"original_scores = []\\ntrial1_scores = []\\n\\nfrom sklearn.model_selection import KFold\\n\\nkf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\\nkf.get_n_splits(x)\\n\\nprint(kf)\\n\\ndata_cv = []\\n\\nfor train_index, test_index in kf.split(x):\\n    # print(\\\"TRAIN:\\\", train_index, \\\"TEST:\\\", test_index)\\n    X_train, X_test = x[train_index], x[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    data_cv.append([[X_train, X_test], [y_train, y_test]])\\n\\n\\nfor i in range(len(data_cv)):\\n    print(\\\"************************** ==> \\\", i)\\n\\n    xtrain = data_cv[i][0][0]\\n    ytrain_original = data_cv[i][1][0]\\n    xtest = real_xtest  # data_cv[i][0][1]\\n    ytest_original = real_ytest  # data_cv[i][1][1]\\n\\n    ytrain = ytrain_original.copy()\\n    ytest = ytest_original.copy()\\n\\n    # member values\\n    clf = []\\n    acc = []\\n    finalacc = []\\n    ypredproba_all = []\\n    ypredconfprob_all = []\\n\\n    # orginal score using random forest classifier\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    print(\\\"original score\\\", m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n\\n    # generate three base classifers using RF,svm and XGBoost\\n\\n    # =================================================\\n\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    rfpred = rf.predict(xtest)\\n    print(\\\"RF f1_score\\\", m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n    clf.append(rf)\\n    acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(rf.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, rfpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    svc = svmgpu(random_state=randomseed, probability=True, C=100, gamma=0.0001)\\n    svc.fit(xtrain, ytrain)\\n\\n    svcpred = svc.predict(xtest)\\n    print(\\\"RF svc\\\", m.f1_score(ytest, svcpred, average=\\\"weighted\\\"))\\n\\n    clf.append(svc)\\n    acc.append(m.f1_score(ytest, svcpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(svc.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, svcpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    xgbc = xgb.XGBClassifier(random_state=randomseed, n_estimators=100)\\n    xgbc.fit(xtrain, ytrain)\\n\\n    xgbpred = xgbc.predict(xtest)\\n    print(\\\"RF xgbc\\\", m.f1_score(ytest, xgbpred, average=\\\"weighted\\\"))\\n\\n    clf.append(xgbc)\\n    acc.append(m.f1_score(ytest, xgbpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(xgbc.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, xgbpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    # =================================================\\n    # generate combinations of features 12,7\\n    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\\n\\n    # generate 10 random numbers\\n    randnums = []\\n    for i in range(10):\\n        randnums.append(random.randrange(0, len(comb)))\\n\\n    print(randnums)\\n\\n    comb = np.array(comb)[randnums, :]\\n\\n    for i in range(len(comb)):\\n        print(i, \\\" ==================== \\\", comb[i])\\n\\n        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\\n        rf.fit(xtrain[:, comb[i]], ytrain)\\n        rfpred = rf.predict(xtest[:, comb[i]])\\n        print(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n        clf.append(rf)\\n        acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\\n\\n        confmat = m.confusion_matrix(ytest, rfpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\\n        xgbmodel.fit(xtrain, ytrain)\\n        xgbmodelpred = xgbmodel.predict(xtest)\\n        print(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n\\n        clf.append(xgbmodel)\\n        acc.append(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(xgbmodel.predict_proba(xtest))\\n\\n        confmat = m.confusion_matrix(ytest, xgbmodelpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n    # #=================================================\\n    # Compute the weight using ga and compute the ensemble accuracy\\n    import calculateWeightUsingGa2 as aresult\\n\\n    weightvalga = aresult.getbestvalues(acc)\\n\\n    finalval = 0\\n    for i in range(len(acc)):\\n        finalval += weightvalga[i] * ypredproba_all[i]\\n\\n    print(\\n        \\\"f1_score\\\", m.f1_score(ytest, np.argmax(finalval, axis=1), average=\\\"weighted\\\")\\n    )\\n    print(\\\"accuracy_score\\\", m.accuracy_score(ytest, np.argmax(finalval, axis=1)))\\n    trial1_scores.append(\\n        m.f1_score(ytest, np.argmax(finalval, axis=1), average=\\\"weighted\\\")\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_scores = []\n",
    "trial1_scores = [] \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\n",
    "kf.get_n_splits(x)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "data_cv = []\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    data_cv.append([[X_train, X_test], [y_train, y_test]])\n",
    "\n",
    "\n",
    "for i in range(len(data_cv)):\n",
    "    print(\"************************** ==> \", i)\n",
    "    \n",
    "    xtrain = data_cv[i][0][0]\n",
    "    ytrain_original = data_cv[i][1][0]    \n",
    "    xtest = real_xtest  # data_cv[i][0][1]\n",
    "    ytest_original = real_ytest  # data_cv[i][1][1]    \n",
    "    \n",
    "    ytrain=ytrain_original.copy()\n",
    "    ytest=ytest_original.copy() \n",
    "    \n",
    "    # member values\n",
    "    clf = []\n",
    "    acc = []\n",
    "    finalacc = []\n",
    "    ypredproba_all = []\n",
    "    ypredconfprob_all = []\n",
    "    \n",
    "    # orginal score using random forest classifier\n",
    "    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    print(\"original score\", m.f1_score(ytest, rf.predict(xtest), average=\"weighted\"))\n",
    "    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\"weighted\"))\n",
    "    \n",
    "    \n",
    "    # generate three base classifers using RF,svm and XGBoost\n",
    "\n",
    "    #================================================= \n",
    "\n",
    "    rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    rfpred=rf.predict(xtest)\n",
    "    print('RF f1_score',m.f1_score(ytest,rfpred,average='weighted'))\n",
    "\n",
    "    clf.append(rf)\n",
    "    acc.append(m.f1_score(ytest,rfpred,average='weighted'))\n",
    "    ypredproba_all.append(rf.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,rfpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    #=================================================\n",
    "    svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\n",
    "    svc.fit(xtrain,ytrain)\n",
    "\n",
    "    svcpred=svc.predict(xtest)\n",
    "    print('RF svc',m.f1_score(ytest,svcpred,average='weighted'))\n",
    "\n",
    "    clf.append(svc)\n",
    "    acc.append(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "    ypredproba_all.append(svc.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,svcpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    #=================================================\n",
    "    xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\n",
    "    xgbc.fit(xtrain,ytrain)\n",
    "\n",
    "    xgbpred=xgbc.predict(xtest)\n",
    "    print('RF xgbc',m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "\n",
    "\n",
    "    clf.append(xgbc)\n",
    "    acc.append(m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "    ypredproba_all.append(xgbc.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,xgbpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    # =================================================\n",
    "    # =================================================\n",
    "    # generate combinations of features 12,7\n",
    "    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\n",
    "\n",
    "    # generate 10 random numbers\n",
    "    randnums = []\n",
    "    for i in range(10):\n",
    "        randnums.append(random.randrange(0, len(comb)))\n",
    "\n",
    "    print(randnums)\n",
    "\n",
    "    comb = np.array(comb)[randnums, :]\n",
    "\n",
    "\n",
    "    for i in range(len(comb)):\n",
    "        print(i, \" ==================== \", comb[i])\n",
    "\n",
    "        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\n",
    "        rf.fit(xtrain[:, comb[i]], ytrain)\n",
    "        rfpred = rf.predict(xtest[:, comb[i]])\n",
    "        print(m.f1_score(ytest, rfpred, average=\"weighted\"))\n",
    "\n",
    "        clf.append(rf)\n",
    "        acc.append(m.f1_score(ytest, rfpred, average=\"weighted\"))\n",
    "        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\n",
    "\n",
    "        confmat = m.confusion_matrix(ytest, rfpred)\n",
    "        confsumh = np.sum(confmat, axis=0)\n",
    "        propconfmat = confmat.copy()\n",
    "        for i in range(propconfmat.shape[0]):\n",
    "            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "        ypredconfprob_all.append(propconfmat / 100)\n",
    "\n",
    "        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\n",
    "        xgbmodel.fit(xtrain, ytrain)\n",
    "        xgbmodelpred = xgbmodel.predict(xtest)\n",
    "        print(m.f1_score(ytest, xgbmodelpred, average=\"weighted\"))\n",
    "\n",
    "        clf.append(xgbmodel)\n",
    "        acc.append(m.f1_score(ytest, xgbmodelpred, average=\"weighted\"))\n",
    "        ypredproba_all.append(xgbmodel.predict_proba(xtest))\n",
    "\n",
    "        confmat = m.confusion_matrix(ytest, xgbmodelpred)\n",
    "        confsumh = np.sum(confmat, axis=0)\n",
    "        propconfmat = confmat.copy()\n",
    "        for i in range(propconfmat.shape[0]):\n",
    "            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "        ypredconfprob_all.append(propconfmat / 100)\n",
    "\n",
    "    # #=================================================\n",
    "    #Compute the weight using ga and compute the ensemble accuracy\n",
    "    import calculateWeightUsingGa2 as aresult\n",
    "    weightvalga=aresult.getbestvalues(acc)\n",
    "\n",
    "    finalval=0\n",
    "    for i in range(len(acc)):\n",
    "        finalval += weightvalga[i]*ypredproba_all[i]\n",
    "\n",
    "    print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "    print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\n",
    "    trial1_scores.append(m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original socre  0.6786616718532941  std  0.015219306544086725\n",
      "ga socre  0.7375235645018933  std  0.0032792898703312375\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"print(\\\"original socre \\\", np.mean(original_scores), \\\" std \\\", np.std(original_scores))\\nprint(\\\"ga socre \\\", np.mean(trial1_scores), \\\" std \\\", np.std(trial1_scores))\";\n",
       "                var nbb_formatted_code = \"print(\\\"original socre \\\", np.mean(original_scores), \\\" std \\\", np.std(original_scores))\\nprint(\\\"ga socre \\\", np.mean(trial1_scores), \\\" std \\\", np.std(trial1_scores))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"original socre \", np.mean(original_scores), \" std \", np.std(original_scores))\n",
    "print(\"ga socre \", np.mean(trial1_scores), \" std \", np.std(trial1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. voting classifier\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corssvals  0.7629273360121691  std  0.03748535200655878\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"ensemb_clf = []\\nfor i in range(len(clf)):\\n    ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\\n\\neclf3 = VotingClassifier(estimators=ensemb_clf, voting=\\\"soft\\\", flatten_transform=True)\\n\\nfrom sklearn.model_selection import cross_val_score\\ncorssvals=cross_val_score(eclf3, x, y, cv=10)\\nprint(\\\"corssvals \\\", np.mean(corssvals), \\\" std \\\", np.std(corssvals))\";\n",
       "                var nbb_formatted_code = \"ensemb_clf = []\\nfor i in range(len(clf)):\\n    ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\\n\\neclf3 = VotingClassifier(estimators=ensemb_clf, voting=\\\"soft\\\", flatten_transform=True)\\n\\nfrom sklearn.model_selection import cross_val_score\\n\\ncorssvals = cross_val_score(eclf3, x, y, cv=10)\\nprint(\\\"corssvals \\\", np.mean(corssvals), \\\" std \\\", np.std(corssvals))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemb_clf = []\n",
    "for i in range(len(clf)):\n",
    "    ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=ensemb_clf, voting=\"soft\", flatten_transform=True)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "corssvals = cross_val_score(eclf3, x, y, cv=10)\n",
    "print(\"corssvals \", np.mean(corssvals), \" std \", np.std(corssvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
