{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nfrom thundersvm import SVC as svmgpu\\nimport calculateWeightUsingGa2 as aresult\\nimport pandas as pd\\nimport itertools\\nimport random\\nfrom sklearn.ensemble import VotingClassifier\\n\\nrandomseed = 42\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nfrom thundersvm import SVC as svmgpu\\nimport calculateWeightUsingGa2 as aresult\\nimport pandas as pd\\nimport itertools\\nimport random\\nfrom sklearn.ensemble import VotingClassifier\\n\\nrandomseed = 42\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics as m\n",
    "from thundersvm import SVC as svmgpu\n",
    "import calculateWeightUsingGa2 as aresult\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "randomseed = 42\n",
    "np.random.seed(randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Dataset \n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"xtest = np.array(pd.read_csv(\\\"../dataset/xtest.txt\\\"))\\nxtrain = np.array(pd.read_csv(\\\"../dataset/xtrain.txt\\\"))\\nytest_original = np.array(pd.read_csv(\\\"../dataset/ytest.txt\\\")).ravel()\\nytrain_original = np.array(pd.read_csv(\\\"../dataset/ytrain.txt\\\")).ravel()\\n\\nytrain = ytrain_original.copy()\\nytest = ytest_original.copy()\\n\\n\\nreal_xtest=xtest\\nreal_ytest=ytest_original\\n\\nx=xtrain\\ny=ytrain_original\";\n",
       "                var nbb_formatted_code = \"xtest = np.array(pd.read_csv(\\\"../dataset/xtest.txt\\\"))\\nxtrain = np.array(pd.read_csv(\\\"../dataset/xtrain.txt\\\"))\\nytest_original = np.array(pd.read_csv(\\\"../dataset/ytest.txt\\\")).ravel()\\nytrain_original = np.array(pd.read_csv(\\\"../dataset/ytrain.txt\\\")).ravel()\\n\\nytrain = ytrain_original.copy()\\nytest = ytest_original.copy()\\n\\n\\nreal_xtest = xtest\\nreal_ytest = ytest_original\\n\\nx = xtrain\\ny = ytrain_original\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtest = np.array(pd.read_csv(\"../dataset/xtest.txt\"))\n",
    "xtrain = np.array(pd.read_csv(\"../dataset/xtrain.txt\"))\n",
    "ytest_original = np.array(pd.read_csv(\"../dataset/ytest.txt\")).ravel()\n",
    "ytrain_original = np.array(pd.read_csv(\"../dataset/ytrain.txt\")).ravel()\n",
    "\n",
    "ytrain = ytrain_original.copy()\n",
    "ytest = ytest_original.copy()\n",
    "\n",
    "\n",
    "real_xtest=xtest\n",
    "real_ytest=ytest_original\n",
    "\n",
    "x=xtrain\n",
    "y=ytrain_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=42, shuffle=True)\n",
      "************************** ==>  0\n",
      "original score 0.7637048249937456\n",
      "RF f1_score 0.7637048249937456\n",
      "RF svc 0.6550152455835335\n",
      "RF xgbc 0.7419663394836973\n",
      "[353, 513, 545, 259, 415, 15, 142, 527, 202, 328]\n",
      "0  ====================  [ 0  2  4  7  8  9 11]\n",
      "0.7686563683017452\n",
      "0.7313201718096775\n",
      "1  ====================  [ 1  2  3  5  8  9 10]\n",
      "0.7246944401010287\n",
      "0.7313201718096775\n",
      "2  ====================  [ 1  2  4  5  7  9 10]\n",
      "0.7245835813458326\n",
      "0.7313201718096775\n",
      "3  ====================  [ 0  2  3  4  5  7 10]\n",
      "0.7748883494392745\n",
      "0.7313201718096775\n",
      "4  ====================  [ 0  3  5  6  7  8 11]\n",
      "0.777263708808196\n",
      "0.7313201718096775\n",
      "5  ====================  [0 1 2 3 4 8 9]\n",
      "0.7766296966358786\n",
      "0.7313201718096775\n",
      "6  ====================  [0 1 3 4 6 7 9]\n",
      "0.7692688031143226\n",
      "0.7313201718096775\n",
      "7  ====================  [ 1  2  3  7  8  9 10]\n",
      "0.7174445156361953\n",
      "0.7313201718096775\n",
      "8  ====================  [ 0  1  4  5  6  8 11]\n",
      "0.7812657431696014\n",
      "0.7313201718096775\n",
      "9  ====================  [ 0  2  4  5  6  8 11]\n",
      "0.7751768710886251\n",
      "0.7313201718096775\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.638988246273065 [0.87095604 0.39287383 0.85716144 0.9130502  0.50412103 0.50246016\n",
      " 0.5106006  0.42673256 0.52666727 0.93500882 0.66077888 0.98833486\n",
      " 0.68725652 0.95504738 0.77352086 0.91348659 0.77557052 0.42262061\n",
      " 0.80345253 0.99940217 0.80771611 0.93655012 0.82579185]\n",
      "f1_score 0.7766860208460281\n",
      "accuracy_score 0.7787937743190662\n",
      "************************** ==>  1\n",
      "original score 0.7804791621460564\n",
      "RF f1_score 0.7804791621460564\n",
      "RF svc 0.6602247937286191\n",
      "RF xgbc 0.745266176924534\n",
      "[257, 619, 568, 179, 547, 0, 285, 370, 694, 144]\n",
      "0  ====================  [0 2 3 4 5 7 8]\n",
      "0.7821961999726686\n",
      "0.7330874346561873\n",
      "1  ====================  [ 1  3  4  7  8  9 11]\n",
      "0.7438906594239592\n",
      "0.7330874346561873\n",
      "2  ====================  [ 1  2  5  6  7  8 10]\n",
      "0.7295998260032582\n",
      "0.7330874346561873\n",
      "3  ====================  [ 0  1  3  5  8 10 11]\n",
      "0.7904931101970001\n",
      "0.7330874346561873\n",
      "4  ====================  [ 1  2  4  5  7 10 11]\n",
      "0.7533969780967654\n",
      "0.7330874346561873\n",
      "5  ====================  [0 1 2 3 4 5 6]\n",
      "0.7849404774209952\n",
      "0.7330874346561873\n",
      "6  ====================  [ 0  2  3  4  8 10 11]\n",
      "0.7860941937218132\n",
      "0.7330874346561873\n",
      "7  ====================  [ 0  2  5  7  9 10 11]\n",
      "0.7801015321855601\n",
      "0.7330874346561873\n",
      "8  ====================  [ 2  3  4  6  7  8 11]\n",
      "0.7231696797387277\n",
      "0.7330874346561873\n",
      "9  ====================  [ 0  1  3  4  6  7 11]\n",
      "0.786720284897622\n",
      "0.7330874346561873\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.861847114022371 [0.92667962 0.23297432 0.86506581 0.9298932  0.48384282 0.83048281\n",
      " 0.65183637 0.37911672 0.696111   0.99248517 0.69875419 0.91171494\n",
      " 0.72847409 0.93129037 0.73792059 0.94661647 0.74533893 0.91753437\n",
      " 0.75698004 0.28068559 0.77102544 0.97836655 0.79611163]\n",
      "f1_score 0.7905647110193087\n",
      "accuracy_score 0.7924124513618677\n",
      "************************** ==>  2\n",
      "original score 0.7832759585759997\n",
      "RF f1_score 0.7832759585759997\n",
      "RF svc 0.6650960326021228\n",
      "RF xgbc 0.7508462638354708\n",
      "[214, 390, 192, 788, 323, 775, 722, 51, 593, 311]\n",
      "0  ====================  [ 0  1  4  5  8 10 11]\n",
      "0.7942226444300313\n",
      "0.7454047785068338\n",
      "1  ====================  [ 0  3  4  5  7  8 11]\n",
      "0.8012882054655247\n",
      "0.7454047785068338\n",
      "2  ====================  [ 0  1  3  7  8  9 11]\n",
      "0.7906621478250166\n",
      "0.7454047785068338\n",
      "3  ====================  [ 4  5  6  8  9 10 11]\n",
      "0.7047717468088003\n",
      "0.7454047785068338\n",
      "4  ====================  [0 2 4 5 6 7 9]\n",
      "0.7835936967491486\n",
      "0.7454047785068338\n",
      "5  ====================  [ 3  4  6  8  9 10 11]\n",
      "0.7046529437015912\n",
      "0.7454047785068338\n",
      "6  ====================  [ 2  3  6  7  8  9 10]\n",
      "0.649677963954915\n",
      "0.7454047785068338\n",
      "7  ====================  [ 0  1  2  3  7 10 11]\n",
      "0.7962226503214495\n",
      "0.7454047785068338\n",
      "8  ====================  [ 1  3  4  5  6  8 10]\n",
      "0.7402980911110204\n",
      "0.7454047785068338\n",
      "9  ====================  [ 0  2  3  6  7  9 11]\n",
      "0.7925855518042485\n",
      "0.7454047785068338\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.510007248313535 [0.90894588 0.29382726 0.87863659 0.97848789 0.62469981 0.98389849\n",
      " 0.65411616 0.94994141 0.65508517 0.48072894 0.70952909 0.9363992\n",
      " 0.71379358 0.34717959 0.74737995 0.         0.78228083 0.98197533\n",
      " 0.83314406 0.57439716 0.84881839 0.9733294  0.87602255]\n",
      "f1_score 0.7950527192803555\n",
      "accuracy_score 0.7972762645914397\n",
      "************************** ==>  3\n",
      "original score 0.7714700139773595\n",
      "RF f1_score 0.7714700139773595\n",
      "RF svc 0.6590418676309127\n",
      "RF xgbc 0.7370745598917611\n",
      "[34, 410, 585, 552, 549, 735, 469, 352, 157, 512]\n",
      "0  ====================  [ 0  1  2  3  5  9 11]\n",
      "0.7821067232934726\n",
      "0.7280663531453986\n",
      "1  ====================  [ 0  3  4  7  8 10 11]\n",
      "0.7756915994159703\n",
      "0.7280663531453986\n",
      "2  ====================  [ 1  2  6  7  9 10 11]\n",
      "0.7259474224156252\n",
      "0.7280663531453986\n",
      "3  ====================  [1 2 4 6 7 8 9]\n",
      "0.7333633508175783\n",
      "0.7280663531453986\n",
      "4  ====================  [ 1  2  4  5  8  9 11]\n",
      "0.7438136580799224\n",
      "0.7280663531453986\n",
      "5  ====================  [ 2  4  5  6  8  9 11]\n",
      "0.7299347600275201\n",
      "0.7280663531453986\n",
      "6  ====================  [ 1  2  3  4  5  7 10]\n",
      "0.7459045928862839\n",
      "0.7280663531453986\n",
      "7  ====================  [ 0  2  4  7  8  9 10]\n",
      "0.7617573573133319\n",
      "0.7280663531453986\n",
      "8  ====================  [ 0  1  3  4  8  9 10]\n",
      "0.7723965571440633\n",
      "0.7280663531453986\n",
      "9  ====================  [ 1  2  3  5  7 10 11]\n",
      "0.7495238309894707\n",
      "0.7280663531453986\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11.99637297420071 [0.934587   0.18608177 0.83043762 0.99610924 0.39068618 0.98546781\n",
      " 0.4058599  0.26274715 0.47534102 0.80001715 0.53364235 0.85671117\n",
      " 0.62447806 0.78644637 0.63198356 0.90505538 0.6468153  0.93316501\n",
      " 0.66007847 0.97527241 0.73337999 0.9204174  0.77489605]\n",
      "f1_score 0.7767960620610598\n",
      "accuracy_score 0.7797236816501265\n",
      "************************** ==>  4\n",
      "original score 0.7823082927048491\n",
      "RF f1_score 0.7823082927048491\n",
      "RF svc 0.6608897076864041\n",
      "RF xgbc 0.7473051696368431\n",
      "[344, 574, 130, 71, 265, 572, 182, 626, 725, 480]\n",
      "0  ====================  [ 0  2  4  6  7  8 11]\n",
      "0.7813394200580828\n",
      "0.7367671071477623\n",
      "1  ====================  [ 1  2  5  6  8  9 11]\n",
      "0.7508128620288494\n",
      "0.7367671071477623\n",
      "2  ====================  [ 0  1  3  4  5  6 11]\n",
      "0.7932521945279489\n",
      "0.7367671071477623\n",
      "3  ====================  [0 1 2 4 6 7 8]\n",
      "0.7840367460504215\n",
      "0.7367671071477623\n",
      "4  ====================  [ 0  2  3  4  5  9 11]\n",
      "0.7897304544392464\n",
      "0.7367671071477623\n",
      "5  ====================  [ 1  2  5  6  7 10 11]\n",
      "0.7486444694206796\n",
      "0.7367671071477623\n",
      "6  ====================  [ 0  1  3  6  7  8 10]\n",
      "0.7727450553805137\n",
      "0.7367671071477623\n",
      "7  ====================  [ 1  3  5  6  7  9 10]\n",
      "0.7300453115828948\n",
      "0.7367671071477623\n",
      "8  ====================  [ 2  3  6  7  9 10 11]\n",
      "0.7052508095251618\n",
      "0.7367671071477623\n",
      "9  ====================  [ 1  2  3  4  6  7 11]\n",
      "0.7488409623763239\n",
      "0.7367671071477623\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.6347990345397 [0.94433346 0.22308606 0.85642973 0.93746402 0.4071726  0.89159109\n",
      " 0.461723   0.99988864 0.5488506  0.94749268 0.57040353 0.99669601\n",
      " 0.71908468 0.88397627 0.78152496 0.93475496 0.78731789 0.40233573\n",
      " 0.79045822 0.30144043 0.8140595  0.88691004 0.84969467]\n",
      "f1_score 0.787636409343745\n",
      "accuracy_score 0.7898423817863398\n",
      "************************** ==>  5\n",
      "original score 0.7783884694972784\n",
      "RF f1_score 0.7783884694972784\n",
      "RF svc 0.661516873170225\n",
      "RF xgbc 0.7434779754735522\n",
      "[159, 492, 3, 778, 329, 655, 598, 317, 100, 686]\n",
      "0  ====================  [ 0  1  3  4  8 10 11]\n",
      "0.7877236609077385\n",
      "0.736390793714502\n",
      "1  ====================  [ 1  2  3  4  7 10 11]\n",
      "0.7532428358461714\n",
      "0.736390793714502\n",
      "2  ====================  [0 1 2 3 4 5 9]\n",
      "0.7875019002815662\n",
      "0.736390793714502\n",
      "3  ====================  [ 3  5  6  7  8  9 11]\n",
      "0.7228807491457448\n",
      "0.736390793714502\n",
      "4  ====================  [ 0  2  4  5  6  9 10]\n",
      "0.7735260611631337\n",
      "0.736390793714502\n",
      "5  ====================  [ 1  4  5  7  8  9 11]\n",
      "0.7473625739866788\n",
      "0.736390793714502\n",
      "6  ====================  [1 3 4 5 7 8 9]\n",
      "0.7556323308822086\n",
      "0.736390793714502\n",
      "7  ====================  [ 0  2  3  7  8  9 10]\n",
      "0.7761283203511411\n",
      "0.736390793714502\n",
      "8  ====================  [ 0  1  2  5  6 10 11]\n",
      "0.7940613444011585\n",
      "0.736390793714502\n",
      "9  ====================  [ 2  3  4  5  7  9 11]\n",
      "0.7338038006764698\n",
      "0.736390793714502\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.065093335066036 [0.94903033 0.21092853 0.83552911 0.99326295 0.43108181 0.8511475\n",
      " 0.46921788 0.95057463 0.57492618 0.24022113 0.60547214 0.88389663\n",
      " 0.60967671 0.84076307 0.63264238 0.85397803 0.64843682 0.91605285\n",
      " 0.68776324 0.99573354 0.75758188 0.42714293 0.76226338]\n",
      "f1_score 0.7902634125490685\n",
      "accuracy_score 0.7927612375948628\n",
      "************************** ==>  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original score 0.7824638672887874\n",
      "RF f1_score 0.7824638672887874\n",
      "RF svc 0.6600698175043455\n",
      "RF xgbc 0.7433289474536209\n",
      "[669, 234, 23, 122, 685, 363, 278, 622, 272, 59]\n",
      "0  ====================  [ 1  5  6  8  9 10 11]\n",
      "0.7400548928953233\n",
      "0.7368120611758339\n",
      "1  ====================  [ 0  1  5  6  7  9 10]\n",
      "0.7689014005588992\n",
      "0.7368120611758339\n",
      "2  ====================  [0 1 2 3 5 6 9]\n",
      "0.7845382334208353\n",
      "0.7368120611758339\n",
      "3  ====================  [ 0  1  2  7  8  9 11]\n",
      "0.7826072718472763\n",
      "0.7368120611758339\n",
      "4  ====================  [ 2  3  4  5  7  9 10]\n",
      "0.7064472553715074\n",
      "0.7368120611758339\n",
      "5  ====================  [ 0  2  5  6  8  9 10]\n",
      "0.7737768449545857\n",
      "0.7368120611758339\n",
      "6  ====================  [ 0  2  3  4  7  8 10]\n",
      "0.7785879728738868\n",
      "0.7368120611758339\n",
      "7  ====================  [ 1  3  4  8  9 10 11]\n",
      "0.7334134565299625\n",
      "0.7368120611758339\n",
      "8  ====================  [ 0  2  3  4  6  8 10]\n",
      "0.7732735041014894\n",
      "0.7368120611758339\n",
      "9  ====================  [ 0  1  2  4  5  6 10]\n",
      "0.7863503343771611\n",
      "0.7368120611758339\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.558723074125382 [0.96653645 0.40006787 0.88291229 0.73707638 0.53551862 0.89943515\n",
      " 0.55856518 0.96972135 0.56920128 0.96717032 0.59200978 0.44088877\n",
      " 0.59674664 0.93855509 0.60499833 0.96220023 0.6811085  0.4988385\n",
      " 0.68326357 0.9009981  0.70234033 0.9735744  0.72380879]\n",
      "f1_score 0.7886972746932096\n",
      "accuracy_score 0.7917882856586884\n",
      "************************** ==>  7\n",
      "original score 0.7799738684219458\n",
      "RF f1_score 0.7799738684219458\n",
      "RF svc 0.6620490989457333\n",
      "RF xgbc 0.7472741993264527\n",
      "[296, 725, 481, 468, 678, 353, 328, 352, 559, 677]\n",
      "0  ====================  [ 0  2  3  5  6 10 11]\n",
      "0.7859890262821292\n",
      "0.7386103976422274\n",
      "1  ====================  [ 2  3  6  7  9 10 11]\n",
      "0.6985290546497109\n",
      "0.7386103976422274\n",
      "2  ====================  [1 2 3 4 6 8 9]\n",
      "0.7333455989596358\n",
      "0.7386103976422274\n",
      "3  ====================  [1 2 3 4 5 7 9]\n",
      "0.7479507584791645\n",
      "0.7386103976422274\n",
      "4  ====================  [ 2  3  4  5  6  8 11]\n",
      "0.7388653642126812\n",
      "0.7386103976422274\n",
      "5  ====================  [ 0  2  4  7  8  9 11]\n",
      "0.7823966873585103\n",
      "0.7386103976422274\n",
      "6  ====================  [ 0  2  4  5  6  8 11]\n",
      "0.7827426332587137\n",
      "0.7386103976422274\n",
      "7  ====================  [ 0  2  4  7  8  9 10]\n",
      "0.7644283651957764\n",
      "0.7386103976422274\n",
      "8  ====================  [ 1  2  4  6  8  9 11]\n",
      "0.7353357172152283\n",
      "0.7386103976422274\n",
      "9  ====================  [ 2  3  4  5  6  8 10]\n",
      "0.7070750425006638\n",
      "0.7386103976422274\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.429530787572375 [0.95181743 0.3200283  0.88713169 0.9979206  0.44705749 0.38701517\n",
      " 0.62052604 0.42670783 0.68028178 0.90108641 0.71527515 0.86501754\n",
      " 0.7322823  0.95570845 0.75451703 0.96106568 0.83376878 0.94515191\n",
      " 0.83690425 0.42854463 0.8465344  0.39162624 0.86397139]\n",
      "f1_score 0.7868573777915839\n",
      "accuracy_score 0.7896477913991049\n",
      "************************** ==>  8\n",
      "original score 0.7858004273938313\n",
      "RF f1_score 0.7858004273938313\n",
      "RF svc 0.6584373604169728\n",
      "RF xgbc 0.751068272101719\n",
      "[345, 528, 650, 784, 569, 234, 156, 669, 214, 240]\n",
      "0  ====================  [ 0  2  4  6  7  9 10]\n",
      "0.7671957682250731\n",
      "0.7399277893612328\n",
      "1  ====================  [ 1  2  3  7  8  9 11]\n",
      "0.7483549761535634\n",
      "0.7399277893612328\n",
      "2  ====================  [ 1  4  5  6  8  9 10]\n",
      "0.7271990878975141\n",
      "0.7399277893612328\n",
      "3  ====================  [ 4  5  6  7  8  9 10]\n",
      "0.6661050591752268\n",
      "0.7399277893612328\n",
      "4  ====================  [ 1  2  5  6  7  8 11]\n",
      "0.7505945490887478\n",
      "0.7399277893612328\n",
      "5  ====================  [ 0  1  5  6  7  9 10]\n",
      "0.7694947171170307\n",
      "0.7399277893612328\n",
      "6  ====================  [ 0  1  3  4  7 10 11]\n",
      "0.7963540159300269\n",
      "0.7399277893612328\n",
      "7  ====================  [ 1  5  6  8  9 10 11]\n",
      "0.7377402494524249\n",
      "0.7399277893612328\n",
      "8  ====================  [ 0  1  4  5  8 10 11]\n",
      "0.7920414217247872\n",
      "0.7399277893612328\n",
      "9  ====================  [ 0  1  5  6  9 10 11]\n",
      "0.7866916479125082\n",
      "0.7399277893612328\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.510394574338378 [0.97713561 0.22090946 0.89595091 0.94268935 0.50192226 0.86312291\n",
      " 0.52206714 0.34877901 0.55560498 0.29525464 0.67096476 0.88071153\n",
      " 0.67443712 0.96030712 0.76032276 0.99219789 0.77839979 0.49389615\n",
      " 0.80240805 0.98914734 0.80711889 0.9774031  0.83309346]\n",
      "f1_score 0.7923487288447465\n",
      "accuracy_score 0.7950963222416813\n",
      "************************** ==>  9\n",
      "original score 0.7893874481916276\n",
      "RF f1_score 0.7893874481916276\n",
      "RF svc 0.6468812273839516\n",
      "RF xgbc 0.756542977139043\n",
      "[728, 10, 700, 55, 604, 775, 67, 676, 492, 132]\n",
      "0  ====================  [2 4 5 6 7 8 9]\n",
      "0.6982419094328478\n",
      "0.7444571208722265\n",
      "1  ====================  [ 0  1  2  3  4  6 11]\n",
      "0.7920669750046585\n",
      "0.7444571208722265\n",
      "2  ====================  [ 2  3  4  6  8 10 11]\n",
      "0.7241168602898702\n",
      "0.7444571208722265\n",
      "3  ====================  [ 0  1  2  3  9 10 11]\n",
      "0.7915522159070583\n",
      "0.7444571208722265\n",
      "4  ====================  [ 1  3  4  5  8  9 10]\n",
      "0.7370041921578553\n",
      "0.7444571208722265\n",
      "5  ====================  [ 3  4  6  8  9 10 11]\n",
      "0.6953659056842484\n",
      "0.7444571208722265\n",
      "6  ====================  [ 0  1  2  4  5  8 11]\n",
      "0.797363803100436\n",
      "0.7444571208722265\n",
      "7  ====================  [2 3 4 5 6 8 9]\n",
      "0.7072620172031194\n",
      "0.7444571208722265\n",
      "8  ====================  [ 1  2  3  4  7 10 11]\n",
      "0.7559594027267134\n",
      "0.7444571208722265\n",
      "9  ====================  [0 1 3 4 5 7 9]\n",
      "0.7917573339054815\n",
      "0.7444571208722265\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12.561872108786165 [0.95782519 0.2994913  0.92817687 0.43891941 0.63972728 0.97174435\n",
      " 0.65954691 0.4783108  0.66404673 0.96452514 0.66613006 0.51050429\n",
      " 0.71666512 0.35503377 0.75023232 0.98717728 0.83871457 0.45478281\n",
      " 0.87873858 0.92202908 0.88219796 0.96761175 0.90538007]\n",
      "f1_score 0.7894547323510178\n",
      "accuracy_score 0.7925666472076279\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"original_scores = []\\ntrial1_scores = [] \\n\\nfrom sklearn.model_selection import KFold\\n\\nkf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\\nkf.get_n_splits(x)\\n\\nprint(kf)\\n\\ndata_cv = []\\n\\nfor train_index, test_index in kf.split(x):\\n    # print(\\\"TRAIN:\\\", train_index, \\\"TEST:\\\", test_index)\\n    X_train, X_test = x[train_index], x[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    data_cv.append([[X_train, X_test], [y_train, y_test]])\\n\\n\\nfor i in range(len(data_cv)):\\n    print(\\\"************************** ==> \\\", i)\\n    \\n    xtrain = data_cv[i][0][0]\\n    ytrain_original = data_cv[i][1][0]    \\n    xtest = data_cv[i][0][1]\\n    ytest_original = data_cv[i][1][1]    \\n    \\n    ytrain=ytrain_original.copy()\\n    ytest=ytest_original.copy() \\n    \\n    # member values\\n    clf = []\\n    acc = []\\n    finalacc = []\\n    ypredproba_all = []\\n    ypredconfprob_all = []\\n    \\n    # orginal score using random forest classifier\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    print(\\\"original score\\\", m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    \\n    \\n    # generate three base classifers using RF,svm and XGBoost\\n\\n    #================================================= \\n\\n    rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain,ytrain)\\n    rfpred=rf.predict(xtest)\\n    print('RF f1_score',m.f1_score(ytest,rfpred,average='weighted'))\\n\\n    clf.append(rf)\\n    acc.append(m.f1_score(ytest,rfpred,average='weighted'))\\n    ypredproba_all.append(rf.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,rfpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    #=================================================\\n    svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\\n    svc.fit(xtrain,ytrain)\\n\\n    svcpred=svc.predict(xtest)\\n    print('RF svc',m.f1_score(ytest,svcpred,average='weighted'))\\n\\n    clf.append(svc)\\n    acc.append(m.f1_score(ytest,svcpred,average='weighted'))\\n    ypredproba_all.append(svc.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,svcpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    #=================================================\\n    xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\\n    xgbc.fit(xtrain,ytrain)\\n\\n    xgbpred=xgbc.predict(xtest)\\n    print('RF xgbc',m.f1_score(ytest,xgbpred,average='weighted'))\\n\\n\\n    clf.append(xgbc)\\n    acc.append(m.f1_score(ytest,xgbpred,average='weighted'))\\n    ypredproba_all.append(xgbc.predict_proba(xtest))\\n\\n    confmat=m.confusion_matrix(ytest,xgbpred)\\n    confsumh=np.sum(confmat,axis=1)\\n    propconfmat=confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \\n    ypredconfprob_all.append(propconfmat/100)\\n\\n    # =================================================\\n    # =================================================\\n    # generate combinations of features 12,7\\n    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\\n\\n    # generate 10 random numbers\\n    randnums = []\\n    for i in range(10):\\n        randnums.append(random.randrange(0, len(comb)))\\n\\n    print(randnums)\\n\\n    comb = np.array(comb)[randnums, :]\\n\\n\\n    for i in range(len(comb)):\\n        print(i, \\\" ==================== \\\", comb[i])\\n\\n        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\\n        rf.fit(xtrain[:, comb[i]], ytrain)\\n        rfpred = rf.predict(xtest[:, comb[i]])\\n        print(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n        clf.append(rf)\\n        acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\\n\\n        confmat = m.confusion_matrix(ytest, rfpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\\n        xgbmodel.fit(xtrain, ytrain)\\n        xgbmodelpred = xgbmodel.predict(xtest)\\n        print(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n\\n        clf.append(xgbmodel)\\n        acc.append(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(xgbmodel.predict_proba(xtest))\\n\\n        confmat = m.confusion_matrix(ytest, xgbmodelpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n    # #=================================================\\n    #Compute the weight using ga and compute the ensemble accuracy\\n    import calculateWeightUsingGa2 as aresult\\n    weightvalga=aresult.getbestvalues(acc)\\n\\n    finalval=0\\n    for i in range(len(acc)):\\n        finalval += weightvalga[i]*ypredproba_all[i]\\n\\n    print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\\n    print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\\n    trial1_scores.append(m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\";\n",
       "                var nbb_formatted_code = \"original_scores = []\\ntrial1_scores = []\\n\\nfrom sklearn.model_selection import KFold\\n\\nkf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\\nkf.get_n_splits(x)\\n\\nprint(kf)\\n\\ndata_cv = []\\n\\nfor train_index, test_index in kf.split(x):\\n    # print(\\\"TRAIN:\\\", train_index, \\\"TEST:\\\", test_index)\\n    X_train, X_test = x[train_index], x[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    data_cv.append([[X_train, X_test], [y_train, y_test]])\\n\\n\\nfor i in range(len(data_cv)):\\n    print(\\\"************************** ==> \\\", i)\\n\\n    xtrain = data_cv[i][0][0]\\n    ytrain_original = data_cv[i][1][0]\\n    xtest = data_cv[i][0][1]\\n    ytest_original = data_cv[i][1][1]\\n\\n    ytrain = ytrain_original.copy()\\n    ytest = ytest_original.copy()\\n\\n    # member values\\n    clf = []\\n    acc = []\\n    finalacc = []\\n    ypredproba_all = []\\n    ypredconfprob_all = []\\n\\n    # orginal score using random forest classifier\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    print(\\\"original score\\\", m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\\\"weighted\\\"))\\n\\n    # generate three base classifers using RF,svm and XGBoost\\n\\n    # =================================================\\n\\n    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\\n    rf.fit(xtrain, ytrain)\\n    rfpred = rf.predict(xtest)\\n    print(\\\"RF f1_score\\\", m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n    clf.append(rf)\\n    acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(rf.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, rfpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    svc = svmgpu(random_state=randomseed, probability=True, C=100, gamma=0.0001)\\n    svc.fit(xtrain, ytrain)\\n\\n    svcpred = svc.predict(xtest)\\n    print(\\\"RF svc\\\", m.f1_score(ytest, svcpred, average=\\\"weighted\\\"))\\n\\n    clf.append(svc)\\n    acc.append(m.f1_score(ytest, svcpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(svc.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, svcpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    xgbc = xgb.XGBClassifier(random_state=randomseed, n_estimators=100)\\n    xgbc.fit(xtrain, ytrain)\\n\\n    xgbpred = xgbc.predict(xtest)\\n    print(\\\"RF xgbc\\\", m.f1_score(ytest, xgbpred, average=\\\"weighted\\\"))\\n\\n    clf.append(xgbc)\\n    acc.append(m.f1_score(ytest, xgbpred, average=\\\"weighted\\\"))\\n    ypredproba_all.append(xgbc.predict_proba(xtest))\\n\\n    confmat = m.confusion_matrix(ytest, xgbpred)\\n    confsumh = np.sum(confmat, axis=1)\\n    propconfmat = confmat.copy()\\n    for i in range(propconfmat.shape[0]):\\n        propconfmat[i] = 100 * propconfmat[i] / confsumh[i]\\n    ypredconfprob_all.append(propconfmat / 100)\\n\\n    # =================================================\\n    # =================================================\\n    # generate combinations of features 12,7\\n    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\\n\\n    # generate 10 random numbers\\n    randnums = []\\n    for i in range(10):\\n        randnums.append(random.randrange(0, len(comb)))\\n\\n    print(randnums)\\n\\n    comb = np.array(comb)[randnums, :]\\n\\n    for i in range(len(comb)):\\n        print(i, \\\" ==================== \\\", comb[i])\\n\\n        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\\n        rf.fit(xtrain[:, comb[i]], ytrain)\\n        rfpred = rf.predict(xtest[:, comb[i]])\\n        print(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n\\n        clf.append(rf)\\n        acc.append(m.f1_score(ytest, rfpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\\n\\n        confmat = m.confusion_matrix(ytest, rfpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\\n        xgbmodel.fit(xtrain, ytrain)\\n        xgbmodelpred = xgbmodel.predict(xtest)\\n        print(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n\\n        clf.append(xgbmodel)\\n        acc.append(m.f1_score(ytest, xgbmodelpred, average=\\\"weighted\\\"))\\n        ypredproba_all.append(xgbmodel.predict_proba(xtest))\\n\\n        confmat = m.confusion_matrix(ytest, xgbmodelpred)\\n        confsumh = np.sum(confmat, axis=0)\\n        propconfmat = confmat.copy()\\n        for i in range(propconfmat.shape[0]):\\n            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\\n        ypredconfprob_all.append(propconfmat / 100)\\n\\n    # #=================================================\\n    # Compute the weight using ga and compute the ensemble accuracy\\n    import calculateWeightUsingGa2 as aresult\\n\\n    weightvalga = aresult.getbestvalues(acc)\\n\\n    finalval = 0\\n    for i in range(len(acc)):\\n        finalval += weightvalga[i] * ypredproba_all[i]\\n\\n    print(\\n        \\\"f1_score\\\", m.f1_score(ytest, np.argmax(finalval, axis=1), average=\\\"weighted\\\")\\n    )\\n    print(\\\"accuracy_score\\\", m.accuracy_score(ytest, np.argmax(finalval, axis=1)))\\n    trial1_scores.append(\\n        m.f1_score(ytest, np.argmax(finalval, axis=1), average=\\\"weighted\\\")\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_scores = []\n",
    "trial1_scores = [] \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=randomseed, shuffle=True)\n",
    "kf.get_n_splits(x)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "data_cv = []\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    data_cv.append([[X_train, X_test], [y_train, y_test]])\n",
    "\n",
    "\n",
    "for i in range(len(data_cv)):\n",
    "    print(\"************************** ==> \", i)\n",
    "    \n",
    "    xtrain = data_cv[i][0][0]\n",
    "    ytrain_original = data_cv[i][1][0]    \n",
    "    xtest = data_cv[i][0][1]\n",
    "    ytest_original = data_cv[i][1][1]    \n",
    "    \n",
    "    ytrain=ytrain_original.copy()\n",
    "    ytest=ytest_original.copy() \n",
    "    \n",
    "    # member values\n",
    "    clf = []\n",
    "    acc = []\n",
    "    finalacc = []\n",
    "    ypredproba_all = []\n",
    "    ypredconfprob_all = []\n",
    "    \n",
    "    # orginal score using random forest classifier\n",
    "    rf = RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    print(\"original score\", m.f1_score(ytest, rf.predict(xtest), average=\"weighted\"))\n",
    "    original_scores.append(m.f1_score(ytest, rf.predict(xtest), average=\"weighted\"))\n",
    "    \n",
    "    \n",
    "    # generate three base classifers using RF,svm and XGBoost\n",
    "\n",
    "    #================================================= \n",
    "\n",
    "    rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    rfpred=rf.predict(xtest)\n",
    "    print('RF f1_score',m.f1_score(ytest,rfpred,average='weighted'))\n",
    "\n",
    "    clf.append(rf)\n",
    "    acc.append(m.f1_score(ytest,rfpred,average='weighted'))\n",
    "    ypredproba_all.append(rf.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,rfpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    #=================================================\n",
    "    svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\n",
    "    svc.fit(xtrain,ytrain)\n",
    "\n",
    "    svcpred=svc.predict(xtest)\n",
    "    print('RF svc',m.f1_score(ytest,svcpred,average='weighted'))\n",
    "\n",
    "    clf.append(svc)\n",
    "    acc.append(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "    ypredproba_all.append(svc.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,svcpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    #=================================================\n",
    "    xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\n",
    "    xgbc.fit(xtrain,ytrain)\n",
    "\n",
    "    xgbpred=xgbc.predict(xtest)\n",
    "    print('RF xgbc',m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "\n",
    "\n",
    "    clf.append(xgbc)\n",
    "    acc.append(m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "    ypredproba_all.append(xgbc.predict_proba(xtest))\n",
    "\n",
    "    confmat=m.confusion_matrix(ytest,xgbpred)\n",
    "    confsumh=np.sum(confmat,axis=1)\n",
    "    propconfmat=confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[i]= 100*propconfmat[i]/confsumh[i] \n",
    "    ypredconfprob_all.append(propconfmat/100)\n",
    "\n",
    "    # =================================================\n",
    "    # =================================================\n",
    "    # generate combinations of features 12,7\n",
    "    comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\n",
    "\n",
    "    # generate 10 random numbers\n",
    "    randnums = []\n",
    "    for i in range(10):\n",
    "        randnums.append(random.randrange(0, len(comb)))\n",
    "\n",
    "    print(randnums)\n",
    "\n",
    "    comb = np.array(comb)[randnums, :]\n",
    "\n",
    "\n",
    "    for i in range(len(comb)):\n",
    "        print(i, \" ==================== \", comb[i])\n",
    "\n",
    "        rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\n",
    "        rf.fit(xtrain[:, comb[i]], ytrain)\n",
    "        rfpred = rf.predict(xtest[:, comb[i]])\n",
    "        print(m.f1_score(ytest, rfpred, average=\"weighted\"))\n",
    "\n",
    "        clf.append(rf)\n",
    "        acc.append(m.f1_score(ytest, rfpred, average=\"weighted\"))\n",
    "        ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]]))\n",
    "\n",
    "        confmat = m.confusion_matrix(ytest, rfpred)\n",
    "        confsumh = np.sum(confmat, axis=0)\n",
    "        propconfmat = confmat.copy()\n",
    "        for i in range(propconfmat.shape[0]):\n",
    "            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "        ypredconfprob_all.append(propconfmat / 100)\n",
    "\n",
    "        xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\n",
    "        xgbmodel.fit(xtrain, ytrain)\n",
    "        xgbmodelpred = xgbmodel.predict(xtest)\n",
    "        print(m.f1_score(ytest, xgbmodelpred, average=\"weighted\"))\n",
    "\n",
    "        clf.append(xgbmodel)\n",
    "        acc.append(m.f1_score(ytest, xgbmodelpred, average=\"weighted\"))\n",
    "        ypredproba_all.append(xgbmodel.predict_proba(xtest))\n",
    "\n",
    "        confmat = m.confusion_matrix(ytest, xgbmodelpred)\n",
    "        confsumh = np.sum(confmat, axis=0)\n",
    "        propconfmat = confmat.copy()\n",
    "        for i in range(propconfmat.shape[0]):\n",
    "            propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "        ypredconfprob_all.append(propconfmat / 100)\n",
    "\n",
    "    # #=================================================\n",
    "    #Compute the weight using ga and compute the ensemble accuracy\n",
    "    import calculateWeightUsingGa2 as aresult\n",
    "    weightvalga=aresult.getbestvalues(acc)\n",
    "\n",
    "    finalval=0\n",
    "    for i in range(len(acc)):\n",
    "        finalval += weightvalga[i]*ypredproba_all[i]\n",
    "\n",
    "    print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "    print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\n",
    "    trial1_scores.append(m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original socre  0.7797252333191481  std  0.006958919713906091\n",
      "ga socre  0.7874357448780124  std  0.005785704830779771\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"print(\\\"original socre \\\", np.mean(original_scores), \\\" std \\\", np.std(original_scores))\\nprint(\\\"ga socre \\\", np.mean(trial1_scores), \\\" std \\\", np.std(trial1_scores))\";\n",
       "                var nbb_formatted_code = \"print(\\\"original socre \\\", np.mean(original_scores), \\\" std \\\", np.std(original_scores))\\nprint(\\\"ga socre \\\", np.mean(trial1_scores), \\\" std \\\", np.std(trial1_scores))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"original socre \", np.mean(original_scores), \" std \", np.std(original_scores))\n",
    "print(\"ga socre \", np.mean(trial1_scores), \" std \", np.std(trial1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. voting classifier\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# ensemb_clf = []\\n# for i in range(len(clf)):\\n#     ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\\n\\n# eclf3 = VotingClassifier(estimators=ensemb_clf, voting=\\\"soft\\\", flatten_transform=True)\\n# eclf3 = eclf3.fit(xtrain, ytrain)\\n# _acc = m.accuracy_score(ytest, eclf3.predict(xtest))\\n# print(_acc)\";\n",
       "                var nbb_formatted_code = \"# ensemb_clf = []\\n# for i in range(len(clf)):\\n#     ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\\n\\n# eclf3 = VotingClassifier(estimators=ensemb_clf, voting=\\\"soft\\\", flatten_transform=True)\\n# eclf3 = eclf3.fit(xtrain, ytrain)\\n# _acc = m.accuracy_score(ytest, eclf3.predict(xtest))\\n# print(_acc)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemb_clf = []\n",
    "for i in range(len(clf)):\n",
    "    ensemb_clf.append([str(type(clf[i])) + str(i), clf[i]])\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=ensemb_clf, voting=\"soft\", flatten_transform=True)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "corssvals=cross_val_score(eclf3, x, y, cv=10)\n",
    "print(\"corssvals \", np.mean(corssvals), \" std \", np.std(corssvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
