{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import  numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb \n",
    "from sklearn import metrics as m\n",
    "from thundersvm import SVC as svmgpu\n",
    "import calculateWeightUsingGa2 as aresult\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def swapcolumns(trainval, testval, coldindexval):\n",
    "    trainval[trainval != coldindexval] = 5\n",
    "    testval[testval != coldindexval] = 5\n",
    "\n",
    "    trainval[trainval == coldindexval] = 0\n",
    "    trainval[trainval == 5] = 1\n",
    "\n",
    "    testval[testval == coldindexval] = 0\n",
    "    testval[testval == 5] = 1\n",
    "\n",
    "    return trainval, testval\n",
    "\n",
    "\n",
    "randomseed=42\n",
    "np.random.seed(randomseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtest=np.array(pd.read_csv('../dataset/xtest.txt'))\n",
    "xtrain=np.array(pd.read_csv('../dataset/xtrain.txt'))\n",
    "ytest_original=np.array(pd.read_csv('../dataset/ytest.txt')).ravel()\n",
    "ytrain_original =np.array(pd.read_csv('../dataset/ytrain.txt')).ravel()\n",
    "\n",
    "# data=datasets.load_wine()\n",
    "# x=data.data\n",
    "# y=data.target\n",
    "\n",
    "# data = pd.read_csv(\"../dataset/seeds_dataset.txt\", sep=\"\\t\", header=None)\n",
    "# data = shuffle(data)\n",
    "# le = LabelEncoder()\n",
    "# data.iloc[:, -1] = le.fit_transform(data.iloc[:, -1])\n",
    "# x = data.iloc[:, :-1]\n",
    "# y = data.iloc[:, -1]\n",
    "# print(np.unique(y))\n",
    "\n",
    "# data = pd.read_csv(\"../dataset/ionosphere.data\",  header=None)\n",
    "# data = shuffle(data)\n",
    "# le = LabelEncoder()\n",
    "# data.iloc[:, -1] = le.fit_transform(data.iloc[:, -1])\n",
    "# x = data.iloc[:, :-1]\n",
    "# y = data.iloc[:, -1]\n",
    "# print(np.unique(y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xtrain,xtest,ytrain_original,ytest_original=train_test_split(x,y,random_state=randomseed,test_size=0.3) \n",
    "\n",
    "ytrain=ytrain_original.copy()\n",
    "ytest=ytest_original.copy() \n",
    "\n",
    "# ytrain, ytest = swapcolumns(ytrain, ytest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original score 0.6765880295918836\n",
      "original score 2 0.7315606204495092\n",
      "0.6765880295918836\n",
      "0.5333333333333333\n",
      "0.7315606204495092\n",
      "[221, 646, 601, 182, 85, 781, 113, 380, 373, 204, 513, 660, 311, 710, 648, 165, 729, 658, 76, 503, 179, 752, 27, 51, 392, 464, 757, 192, 6, 263, 453, 500, 671, 627, 616, 463, 176, 582, 786, 659, 482, 357, 186, 405, 672, 98, 183, 101, 506, 335]\n",
      "0  ====================  [ 0  1  4  6  7 10 11]\n",
      "0.6848088520187307\n",
      "0.738221824835998\n",
      "1  ====================  [ 1  4  5  6  7  8 11]\n",
      "0.664513297603802\n",
      "0.738221824835998\n",
      "2  ====================  [ 1  3  4  5  7  9 10]\n",
      "0.6665071770334928\n",
      "0.738221824835998\n",
      "3  ====================  [ 0  1  3  6  7  8 10]\n",
      "0.7280303850673597\n",
      "0.738221824835998\n",
      "4  ====================  [ 0  1  2  4  7  9 11]\n",
      "0.7145645605497474\n",
      "0.738221824835998\n",
      "5  ====================  [ 3  5  6  8  9 10 11]\n",
      "0.5725761396113839\n",
      "0.738221824835998\n",
      "6  ====================  [ 0  1  2  6  7  8 11]\n",
      "0.7147332386315894\n",
      "0.738221824835998\n",
      "7  ====================  [ 0  3  4  5  6  7 10]\n",
      "0.6848088520187307\n",
      "0.738221824835998\n",
      "8  ====================  [ 0  2  6  7  8  9 11]\n",
      "0.6802147391292281\n",
      "0.738221824835998\n",
      "9  ====================  [ 0  1  4  5  6  9 11]\n",
      "0.7129436588130345\n",
      "0.738221824835998\n",
      "10  ====================  [ 1  2  3  5  8  9 10]\n",
      "0.6556712962962963\n",
      "0.738221824835998\n",
      "11  ====================  [ 1  4  6  7  8  9 11]\n",
      "0.6657722804442252\n",
      "0.738221824835998\n",
      "12  ====================  [ 0  2  3  6  7  9 11]\n",
      "0.689589515839446\n",
      "0.738221824835998\n",
      "13  ====================  [ 2  3  5  6  7  9 10]\n",
      "0.5750152929599192\n",
      "0.738221824835998\n",
      "14  ====================  [ 1  4  5  6  7  9 11]\n",
      "0.6519139935289003\n",
      "0.738221824835998\n",
      "15  ====================  [0 1 3 5 6 8 9]\n",
      "0.6919118977221166\n",
      "0.738221824835998\n",
      "16  ====================  [ 2  4  5  6  7  8 10]\n",
      "0.575234161811343\n",
      "0.738221824835998\n",
      "17  ====================  [ 1  4  5  8  9 10 11]\n",
      "0.6614842597110264\n",
      "0.738221824835998\n",
      "18  ====================  [ 0  1  2  4  6  8 10]\n",
      "0.7086539426964958\n",
      "0.738221824835998\n",
      "19  ====================  [ 1  2  3  5  6  8 11]\n",
      "0.6488271418503976\n",
      "0.738221824835998\n",
      "20  ====================  [ 0  1  3  5  8 10 11]\n",
      "0.6905415424415945\n",
      "0.738221824835998\n",
      "21  ====================  [ 2  5  6  7  9 10 11]\n",
      "0.5540236503417464\n",
      "0.738221824835998\n",
      "22  ====================  [0 1 2 3 5 7 9]\n",
      "0.7327407407407408\n",
      "0.738221824835998\n",
      "23  ====================  [ 0  1  2  3  7 10 11]\n",
      "0.7076724586080455\n",
      "0.738221824835998\n",
      "24  ====================  [ 0  3  4  5  7  9 11]\n",
      "0.6872840849936113\n",
      "0.738221824835998\n",
      "25  ====================  [1 2 3 4 5 6 9]\n",
      "0.6423478683528602\n",
      "0.738221824835998\n",
      "26  ====================  [ 3  4  5  6  7  8 10]\n",
      "0.5590634043672718\n",
      "0.738221824835998\n",
      "27  ====================  [ 0  1  3  7  8  9 11]\n",
      "0.699604366271033\n",
      "0.738221824835998\n",
      "28  ====================  [0 1 2 3 4 6 7]\n",
      "0.7178266798577502\n",
      "0.738221824835998\n",
      "29  ====================  [ 0  2  3  4  5  8 11]\n",
      "0.7027145833760708\n",
      "0.738221824835998\n",
      "30  ====================  [ 0  4  6  8  9 10 11]\n",
      "0.6894528091638514\n",
      "0.738221824835998\n",
      "31  ====================  [ 1  2  3  5  6  7 11]\n",
      "0.6660193389165353\n",
      "0.738221824835998\n",
      "32  ====================  [ 1  6  7  8  9 10 11]\n",
      "0.6139215474777983\n",
      "0.738221824835998\n",
      "33  ====================  [ 1  3  5  6  7  9 11]\n",
      "0.6525842936976924\n",
      "0.738221824835998\n",
      "34  ====================  [ 1  3  4  6  8 10 11]\n",
      "0.66017251231259\n",
      "0.738221824835998\n",
      "35  ====================  [1 2 3 4 5 6 8]\n",
      "0.6727387729285264\n",
      "0.738221824835998\n",
      "36  ====================  [ 0  1  3  5  7 10 11]\n",
      "0.7057416267942583\n",
      "0.738221824835998\n",
      "37  ====================  [ 1  2  6  7  8  9 10]\n",
      "0.6808785025987386\n",
      "0.738221824835998\n",
      "38  ====================  [ 4  5  6  7  8 10 11]\n",
      "0.5317580338442647\n",
      "0.738221824835998\n",
      "39  ====================  [ 1  4  6  7  8  9 10]\n",
      "0.6482270019774113\n",
      "0.738221824835998\n",
      "40  ====================  [ 1  2  3  4  6  8 10]\n",
      "0.6546338219118514\n",
      "0.738221824835998\n",
      "41  ====================  [0 2 5 6 7 8 9]\n",
      "0.6793585733290188\n",
      "0.738221824835998\n",
      "42  ====================  [ 0  1  3  6  7 10 11]\n",
      "0.7251644914248734\n",
      "0.738221824835998\n",
      "43  ====================  [ 0  3  4  6  8  9 11]\n",
      "0.6984322808391088\n",
      "0.738221824835998\n",
      "44  ====================  [2 3 4 5 6 7 8]\n",
      "0.5684251433012726\n",
      "0.738221824835998\n",
      "45  ====================  [ 0  1  2  5  6  9 10]\n",
      "0.726285288151652\n",
      "0.738221824835998\n",
      "46  ====================  [ 0  1  3  6  7  8 11]\n",
      "0.7276130377817589\n",
      "0.738221824835998\n",
      "47  ====================  [0 1 2 5 7 8 9]\n",
      "0.7238190840675314\n",
      "0.738221824835998\n",
      "48  ====================  [ 1  2  3  5  6 10 11]\n",
      "0.6617192331478046\n",
      "0.738221824835998\n",
      "49  ====================  [ 0  2  4  5  7  9 10]\n",
      "0.6832785166118499\n",
      "0.738221824835998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf=[]\n",
    "acc=[]\n",
    "finalacc=[]\n",
    "ypredproba_all=[]\n",
    "ypredconfprob_all=[]\n",
    "\n",
    "\n",
    "#================================================= \n",
    "\n",
    "# Class 1\n",
    "# ===========================\n",
    "ytrain = ytrain_original.copy()\n",
    "ytest = ytest_original.copy()\n",
    "ytrain,ytest= swapcolumns(ytrain,ytest,1)\n",
    "#=================================================\n",
    "\n",
    "rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "rf.fit(xtrain,ytrain)\n",
    "print('original score',m.f1_score(ytest,rf.predict(xtest),average='weighted'))\n",
    "\n",
    "\n",
    "xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\n",
    "xgbc.fit(xtrain,ytrain)\n",
    "xgbpred=xgbc.predict(xtest)\n",
    "print('original score 2',m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "\n",
    "\n",
    "#================================================= \n",
    "\n",
    "rf=RandomForestClassifier(random_state=randomseed, n_estimators=10)\n",
    "rf.fit(xtrain,ytrain)\n",
    "rfpred=rf.predict(xtest)\n",
    "print(m.f1_score(ytest,rfpred,average='weighted'))\n",
    "\n",
    "clf.append(rf)\n",
    "acc.append(m.f1_score(ytest,rfpred,average='weighted'))\n",
    "ypredproba_all.append(rf.predict_proba(xtest)[:,0])\n",
    "\n",
    "confmat = m.confusion_matrix(ytest, rfpred)\n",
    "confsumh = np.sum(confmat, axis=0)\n",
    "propconfmat = confmat.copy()\n",
    "for i in range(propconfmat.shape[0]):\n",
    "    propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "ypredconfprob_all.append(propconfmat.ravel() / 100)\n",
    "\n",
    "\n",
    "#=================================================\n",
    "svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\n",
    "svc.fit(xtrain,ytrain)\n",
    "\n",
    "svcpred=svc.predict(xtest)\n",
    "print(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "\n",
    "clf.append(svc)\n",
    "acc.append(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "ypredproba_all.append(svc.predict_proba(xtest)[:,0])\n",
    "\n",
    "confmat = m.confusion_matrix(ytest, rfpred)\n",
    "confsumh = np.sum(confmat, axis=0)\n",
    "propconfmat = confmat.copy()\n",
    "for i in range(propconfmat.shape[0]):\n",
    "    propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "ypredconfprob_all.append(propconfmat.ravel() / 100)\n",
    "\n",
    "#=================================================\n",
    "xgbc=xgb.XGBClassifier(random_state=randomseed,n_estimators=100)\n",
    "xgbc.fit(xtrain,ytrain)\n",
    "\n",
    "xgbpred=xgbc.predict(xtest)\n",
    "print(m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "\n",
    "\n",
    "clf.append(xgbc)\n",
    "acc.append(m.f1_score(ytest,xgbpred,average='weighted'))\n",
    "ypredproba_all.append(xgbc.predict_proba(xtest)[:,0])\n",
    "\n",
    "confmat = m.confusion_matrix(ytest, rfpred)\n",
    "confsumh = np.sum(confmat, axis=0)\n",
    "propconfmat = confmat.copy()\n",
    "for i in range(propconfmat.shape[0]):\n",
    "    propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "ypredconfprob_all.append(propconfmat.ravel() / 100)\n",
    "\n",
    "#=================================================\n",
    "#=================================================\n",
    "# generate combinations of features 12,6\n",
    "comb = list(itertools.combinations(np.arange(0, 12, 1), 7))\n",
    "\n",
    "# generate 50 random numbers\n",
    "randnums = []\n",
    "for i in range(50):\n",
    "    randnums.append(random.randrange(0, len(comb)))\n",
    "\n",
    "print(randnums)\n",
    "\n",
    "comb = np.array(comb)[randnums, :]\n",
    "\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    print(i, \" ==================== \", comb[i])\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=randomseed, n_estimators=50)\n",
    "    rf.fit(xtrain[:, comb[i]], ytrain)\n",
    "    rfpred = rf.predict(xtest[:, comb[i]])\n",
    "    print(m.f1_score(ytest, rfpred,average='weighted'))\n",
    "\n",
    "    clf.append(rf)\n",
    "    acc.append(m.f1_score(ytest, rfpred,average='weighted'))\n",
    "    ypredproba_all.append(rf.predict_proba(xtest[:, comb[i]])[:,0])\n",
    "\n",
    "    confmat = m.confusion_matrix(ytest, rfpred)\n",
    "    confsumh = np.sum(confmat, axis=0)\n",
    "    propconfmat = confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "    ypredconfprob_all.append(propconfmat.ravel() / 100)\n",
    "\n",
    "    xgbmodel = xgb.XGBClassifier(random_state=randomseed, n_estimators=50)\n",
    "    xgbmodel.fit(xtrain, ytrain)\n",
    "    xgbmodelpred = xgbmodel.predict(xtest)\n",
    "    print(m.f1_score(ytest, xgbmodelpred,average='weighted'))\n",
    "\n",
    "    clf.append(xgbmodel)\n",
    "    acc.append(m.f1_score(ytest, xgbmodelpred,average='weighted'))\n",
    "    ypredproba_all.append(xgbmodel.predict_proba(xtest)[:,0])\n",
    "\n",
    "    confmat = m.confusion_matrix(ytest, xgbmodelpred)\n",
    "    confsumh = np.sum(confmat, axis=0)\n",
    "    propconfmat = confmat.copy()\n",
    "    for i in range(propconfmat.shape[0]):\n",
    "        propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "    ypredconfprob_all.append(propconfmat.ravel() / 100)\n",
    "\n",
    "# #=================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_clf=pd.DataFrame(list(clf))\n",
    "pd_acc=pd.DataFrame(acc) \n",
    "pd_ypredproba_all=pd.DataFrame(np.array(ypredproba_all))\n",
    "pd_ypredconfprob_all=pd.DataFrame(ypredconfprob_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classname='class1.txt'\n",
    "# pd_acc.to_csv('pd_acc_'+classname,sep=',',index=False) \n",
    "# pd_ypredproba_all.to_csv('pd_ypredproba_all_'+classname,sep=',',index=False)\n",
    "# pd_ypredconfprob_all.to_csv('pd_ypredconfprob_all_'+classname,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(ytrain_original).to_csv('ytrain_original.txt',sep=',',index=False)\n",
    "# pd.DataFrame(ytest).to_csv('ytest_class1.txt',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3\n",
       "0    0.50  0.20  0.49  0.79\n",
       "1    0.50  0.20  0.49  0.79\n",
       "2    0.50  0.20  0.49  0.79\n",
       "3    0.51  0.21  0.48  0.78\n",
       "4    0.62  0.20  0.37  0.79\n",
       "..    ...   ...   ...   ...\n",
       "98   0.62  0.20  0.37  0.79\n",
       "99   0.48  0.23  0.51  0.76\n",
       "100  0.62  0.20  0.37  0.79\n",
       "101  0.51  0.21  0.48  0.78\n",
       "102  0.62  0.20  0.37  0.79\n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_ypredconfprob_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "43.93013316097456 [0.41140438 0.01280603 0.67483366 0.48084834 0.68132484 0.3041915\n",
      " 0.68437426 0.36780403 0.68644215 0.66007689 0.70394984 0.59795273\n",
      " 0.70537776 0.12726387 0.70748809 0.5991597  0.71081271 0.48436685\n",
      " 0.72260621 0.441991   0.72703178 0.59179905 0.73623872 0.23044814\n",
      " 0.74260356 0.30724712 0.76318204 0.49651784 0.76382703 0.12932833\n",
      " 0.76782584 0.17102952 0.76932724 0.53123324 0.77018385 0.13909364\n",
      " 0.7812095  0.25079033 0.7827234  0.58202056 0.78928821 0.16567919\n",
      " 0.80023815 0.52911497 0.80434113 0.02319344 0.81248741 0.67897888\n",
      " 0.81324865 0.56987058 0.81557063 0.49568231 0.82558293 0.15439133\n",
      " 0.83814277 0.06593889 0.84075443 0.5486409  0.84447656 0.62960242\n",
      " 0.84597175 0.56534413 0.84905529 0.49595074 0.86556517 0.36081984\n",
      " 0.87212412 0.15148418 0.87227594 0.20815398 0.87804005 0.24691778\n",
      " 0.88432151 0.39337011 0.9066409  0.56679171 0.91183152 0.44595808\n",
      " 0.91720863 0.00819398 0.9212621  0.15840174 0.95086907 0.22200106\n",
      " 0.95319609 0.41228595 0.95491626 0.6483112  0.95589269 0.53977304\n",
      " 0.95810456 0.08374152 0.98380835 0.65062234 0.98623084 0.65144904\n",
      " 0.99030993 0.6426174  0.9913707  0.26836341 0.99759079 0.45401472\n",
      " 0.9979843 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import calculateWeightUsingGa2 as aresult\n",
    "weightvalga=aresult.getbestvalues(acc)\n",
    "\n",
    "temp=pd.DataFrame(ypredproba_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 600)\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(np.array(ypredproba_all).shape)\n",
    "print(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6765880295918836 0.6765880295918836\n",
      "0.16666666666666666 0.5333333333333333\n",
      "0.7315606204495092 0.7315606204495092\n",
      "0.6848088520187307 0.6848088520187307\n",
      "0.738221824835998 0.738221824835998\n",
      "0.664513297603802 0.664513297603802\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6665071770334928 0.6665071770334928\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7280303850673597 0.7280303850673597\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7145645605497474 0.7145645605497474\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5725761396113839 0.5725761396113839\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7147332386315894 0.7147332386315894\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6848088520187307 0.6848088520187307\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6802147391292281 0.6802147391292281\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7129436588130345 0.7129436588130345\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6556712962962963 0.6556712962962963\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6657722804442252 0.6657722804442252\n",
      "0.738221824835998 0.738221824835998\n",
      "0.689589515839446 0.689589515839446\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5750152929599192 0.5750152929599192\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6519139935289003 0.6519139935289003\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6919118977221166 0.6919118977221166\n",
      "0.738221824835998 0.738221824835998\n",
      "0.575234161811343 0.575234161811343\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6614842597110264 0.6614842597110264\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7086539426964958 0.7086539426964958\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6488271418503976 0.6488271418503976\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6905415424415945 0.6905415424415945\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5540236503417464 0.5540236503417464\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7327407407407408 0.7327407407407408\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7076724586080455 0.7076724586080455\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6872840849936113 0.6872840849936113\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6423478683528602 0.6423478683528602\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5590634043672718 0.5590634043672718\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6980678125852618 0.699604366271033\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7178266798577502 0.7178266798577502\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7027145833760708 0.7027145833760708\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6894528091638514 0.6894528091638514\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6660193389165353 0.6660193389165353\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6139215474777983 0.6139215474777983\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6525842936976924 0.6525842936976924\n",
      "0.738221824835998 0.738221824835998\n",
      "0.66017251231259 0.66017251231259\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6727387729285264 0.6727387729285264\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7057416267942583 0.7057416267942583\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6808785025987386 0.6808785025987386\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5317580338442647 0.5317580338442647\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6482270019774113 0.6482270019774113\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6546338219118514 0.6546338219118514\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6793585733290188 0.6793585733290188\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7251644914248734 0.7251644914248734\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6984322808391088 0.6984322808391088\n",
      "0.738221824835998 0.738221824835998\n",
      "0.5684251433012726 0.5684251433012726\n",
      "0.738221824835998 0.738221824835998\n",
      "0.726285288151652 0.726285288151652\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7276130377817589 0.7276130377817589\n",
      "0.738221824835998 0.738221824835998\n",
      "0.7238190840675314 0.7238190840675314\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6617192331478046 0.6617192331478046\n",
      "0.738221824835998 0.738221824835998\n",
      "0.6832785166118499 0.6832785166118499\n",
      "0.738221824835998 0.738221824835998\n",
      "f1_score 0.7456806606962165\n",
      "accuracy_score 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "finalval=0\n",
    "for i in range(len(acc)):\n",
    "    \n",
    "    finalval += weightvalga[i]* np.column_stack( (temp.iloc[i,:],1- temp.iloc[i,:]))\n",
    "        \n",
    "    print(\n",
    "        m.f1_score(ytest, np.argmax(np.column_stack( (temp.iloc[i,:],1- temp.iloc[i,:])), axis=1),average='weighted'),\n",
    "        acc[i],\n",
    "    )\n",
    "    \n",
    "\n",
    "print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalval=[0,]\n",
    "# for i in range(len(acc)):\n",
    "#     finalval += [weightvalga[i]*ypredproba_all[i],weightvalga[i]* (1-ypredproba_all[i])]\n",
    "\n",
    "# print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "# print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.4],\n",
       "       [0.7, 0.4],\n",
       "       [0.2, 0.4],\n",
       "       ...,\n",
       "       [0.8, 0.8],\n",
       "       [0.7, 0.9],\n",
       "       [0.3, 0.7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ypredproba_all[0],(1-ypredproba_all[0])]).reshape(600,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-06db7d8563e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mypredproba_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mypredproba_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "np.column_stack( (ypredproba_all[0,:],1- ypredproba_all[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.DataFrame(ypredproba_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack( (temp.iloc[0,:],1- temp.iloc[0,:])) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalval=0\n",
    "for i in range(len(acc)):\n",
    "    \n",
    "    finalval += ypredproba_all[i]\n",
    "        \n",
    "    print(\n",
    "        m.accuracy_score(ytest, np.argmax(ypredproba_all[i], axis=1)),\n",
    "        acc[i],\n",
    "    )\n",
    "    \n",
    "\n",
    "print('f1_score',m.f1_score(ytest,np.argmax(finalval,axis=1),average='weighted'))\n",
    "print('accuracy_score',m.accuracy_score(ytest,np.argmax(finalval,axis=1)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(ypredproba_all[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "svc=svmgpu(random_state=randomseed,probability=True,C=100,gamma=0.0001)\n",
    "svc.fit(xtrain,ytrain)\n",
    "\n",
    "svcpred=svc.predict(xtest)\n",
    "print(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "\n",
    "temppp=svc.predict_proba(xtest)\n",
    "\n",
    "# clf.append(svc)\n",
    "# acc.append(m.f1_score(ytest,svcpred,average='weighted'))\n",
    "# ypredproba_all.append(svc.predict_proba(xtest))\n",
    "\n",
    "# confmat = m.confusion_matrix(ytest, rfpred)\n",
    "# confsumh = np.sum(confmat, axis=0)\n",
    "# propconfmat = confmat.copy()\n",
    "# for i in range(propconfmat.shape[0]):\n",
    "#     propconfmat[:, i] = 100 * propconfmat[:, i] / confsumh[i]\n",
    "# ypredconfprob_all.append(propconfmat / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(temppp,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72328657, 0.27671343],\n",
       "       [0.65977824, 0.3402218 ],\n",
       "       [0.64820737, 0.35179263],\n",
       "       ...,\n",
       "       [0.68141174, 0.31858823],\n",
       "       [0.69823307, 0.3017669 ],\n",
       "       [0.6632911 , 0.3367089 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.f1_score(ytest, np.argmax(1-temppp, axis=1),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
