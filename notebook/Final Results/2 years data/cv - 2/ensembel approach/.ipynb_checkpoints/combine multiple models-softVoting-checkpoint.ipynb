{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n# !pip install nb_black\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\nimport pandas as pd\\nimport numpy as np\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n# !pip install nb_black\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\nimport pandas as pd\\nimport numpy as np\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "# !pip install nb_black\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multiscorer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ba1b5681b620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSMOTENC\u001b[0m  \u001b[1;31m# doctest: +NORMALIZE_WHITESPACE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmultiscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiScorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'multiscorer'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom sklearn import metrics as m\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nimport xgboost as xgb\\nfrom sklearn.pipeline import make_pipeline\\nfrom mlxtend.feature_selection import ColumnSelector\\nfrom sklearn import model_selection\\nfrom mlxtend.classifier import StackingClassifier\\nfrom thundersvm import SVC as svmgpu\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\\nfrom multiscorer import MultiScorer\\nfrom sklearn.model_selection import cross_val_score\\nfrom numpy import average\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nimport itertools\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom sklearn import metrics as m\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nimport xgboost as xgb\\nfrom sklearn.pipeline import make_pipeline\\nfrom mlxtend.feature_selection import ColumnSelector\\nfrom sklearn import model_selection\\nfrom mlxtend.classifier import StackingClassifier\\nfrom thundersvm import SVC as svmgpu\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\\nfrom multiscorer import MultiScorer\\nfrom sklearn.model_selection import cross_val_score\\nfrom numpy import average\\nimport xgboost as xgb\\nfrom sklearn import metrics as m\\nimport itertools\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics as m\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn import model_selection\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from thundersvm import SVC as svmgpu\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\n",
    "from multiscorer import MultiScorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import average\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics as m\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomseed = 7\n",
    "np.random.seed(randomseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test-temp.csv\")\n",
    "originalFeatures = [\n",
    "    \"FPG\",\n",
    "    \"HbA1c\",\n",
    "    \"Triglyceride\",\n",
    "    \"BMI\",\n",
    "    \"r-GTP gamma\",\n",
    "    \"Uric acid\",\n",
    "    \"Smoking\",\n",
    "    \"Drinking\",\n",
    "    \"Physical activity\",\n",
    "    \"Family history\",\n",
    "    \"SEX\",\n",
    "    \"AGE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=\"Model\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    temp = list(\n",
    "        str(data.iloc[i, 0])\n",
    "        .replace(\"[\", \"\")\n",
    "        .replace(\"]\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .split(\",\")\n",
    "    )\n",
    "    temp_changed = []\n",
    "    for j in range(len(temp)):\n",
    "        temp_changed.append(originalFeatures.index(temp[j].strip()))\n",
    "    data.iloc[i, 0] = str(temp_changed)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list = (\n",
    "#     (data.iloc[0, 0]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\",\")\n",
    "# )\n",
    "# print(test_list)\n",
    "# test_list = [int(i) for i in test_list]\n",
    "# test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Rf_list = (\n",
    "    data.query(\"Model=='RF'\")\n",
    "    .sort_values(by=\"CV_Accuracy\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "xgb_list = (\n",
    "    data.query(\"Model=='XGB'\")\n",
    "    .sort_values(by=\"CV_Accuracy\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "svm_list = (\n",
    "    data.query(\"Model=='SVM'\")\n",
    "    .sort_values(by=\"CV_Accuracy\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ").iloc[:10, :]\n",
    "\n",
    "svm_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2.Read dataset\n",
    " ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data set\n",
    "x_original = pd.read_csv(\"../../../../dataset/XLable_onlyDiabeticRemoved.txt\")\n",
    "\n",
    "conditions = [\n",
    "    (x_original.L100800 < 100),\n",
    "    (x_original.L100800 >= 100) & (x_original.L100800 < 126),\n",
    "    (x_original.L100800 >= 126),\n",
    "]\n",
    "choices = [0, 1, 2]\n",
    "x_original[\"CLASS\"] = np.select(conditions, choices, default=0)\n",
    "x_original = x_original[\n",
    "    [\n",
    "        \"Unnamed: 0\",\n",
    "        \"L100800\",\n",
    "        \"L104600\",\n",
    "        \"L103000\",\n",
    "        \"S000300\",\n",
    "        \"L101700\",\n",
    "        \"L100700\",\n",
    "        \"FIELD_33\",\n",
    "        \"FIELD_38\",\n",
    "        \"FIELD_40\",\n",
    "        \"FIELD_31\",\n",
    "        \"SEX\",\n",
    "        \"AGE\",  #'CLASS',\n",
    "        \"FIELD_16\",\n",
    "        \"FIELD_23\",\n",
    "        \"FIELD_15\",\n",
    "        \"FIELD_22\",\n",
    "        \"FIELD_17\",\n",
    "        \"FIELD_24\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(x_original.shape)\n",
    "\n",
    "y_original = pd.read_csv(\"../../../../dataset/TargetLable_onlyDiabeticRemoved.txt\")\n",
    "\n",
    "conditions = [\n",
    "    (y_original.L100800 < 100),\n",
    "    (y_original.L100800 >= 100) & (y_original.L100800 < 126),\n",
    "    (y_original.L100800 >= 126),\n",
    "]\n",
    "\n",
    "choices = [0, 1, 2]\n",
    "y_original[\"CLASS\"] = np.select(conditions, choices, default=0)\n",
    "\n",
    "y_original = y_original[[\"Unnamed: 0\", \"CLASS\"]]\n",
    "\n",
    "print(y_original.shape)\n",
    "\n",
    "data = pd.merge(\n",
    "    x_original, y_original, how=\"inner\", left_on=\"Unnamed: 0\", right_on=\"Unnamed: 0\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# filter the data set\n",
    "data = data[data.FIELD_16 != 1]  # exclude people who are diagnosed for (diabetes)\n",
    "data = data[data.FIELD_23 != 1]  # exclude people who are on medication for diabetes\n",
    "\n",
    "data = data[\n",
    "    data.FIELD_15 != 1\n",
    "]  # exclude people who are diagnosed for (high blood pressure)\n",
    "data = data[\n",
    "    data.FIELD_22 != 1\n",
    "]  # exclude people who are on medication for high blood pressure\n",
    "\n",
    "data = data[data.FIELD_17 != 1]  # exclude people who are diagnosed for hyperlipidemia\n",
    "data = data[\n",
    "    data.FIELD_24 != 1\n",
    "]  # exclude people who are on medication for hyperlipidemia\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data = data[\n",
    "    [\n",
    "        \"L100800\",\n",
    "        \"L104600\",\n",
    "        \"L103000\",\n",
    "        \"S000300\",\n",
    "        \"L101700\",\n",
    "        \"L100700\",\n",
    "        \"FIELD_33\",\n",
    "        \"FIELD_38\",\n",
    "        \"FIELD_40\",\n",
    "        \"FIELD_31\",\n",
    "        \"SEX\",\n",
    "        \"AGE\",\n",
    "        \"CLASS\",\n",
    "    ]\n",
    "]\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic = data[data.CLASS == 2]\n",
    "prediabetic = data[data.CLASS == 1]\n",
    "normal = data[data.CLASS == 0]\n",
    "\n",
    "print(diabetic.shape[0], prediabetic.shape[0], normal.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_test = diabetic.sample(200, random_state=randomseed)\n",
    "prediabetic_test = prediabetic.sample(200, random_state=randomseed)\n",
    "normal_test = normal.sample(200, random_state=randomseed)\n",
    "test = pd.concat([diabetic_test, prediabetic_test, normal_test])\n",
    "\n",
    "diabetic_train = diabetic.drop(diabetic_test.index)\n",
    "prediabetic_train = prediabetic.drop(prediabetic_test.index)\n",
    "# .sample(\n",
    "#     10 * diabetic_train.shape[0], random_state=randomseed\n",
    "# )\n",
    "normal_train = normal.drop(normal_test.index).sample(\n",
    "    prediabetic_train.shape[0],\n",
    "    random_state=randomseed\n",
    "    #     10 * diabetic_train.shape[0], random_state=randomseed\n",
    ")\n",
    "train = pd.concat([diabetic_train, diabetic_train, prediabetic_train, normal_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train.iloc[:, :-1]\n",
    "ytrain = train.iloc[:, -1]\n",
    "xtest = test.iloc[:, :-1]\n",
    "ytest = test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "randomseed = 42\n",
    "\n",
    "sm = SMOTE(random_state=randomseed, sampling_strategy=\"minority\")\n",
    "X_res, y_res = sm.fit_resample(xtrain, ytrain)\n",
    "\n",
    "print(\"Resampled dataset shape %s\" % Counter(y_res))\n",
    "print(\n",
    "    y_res[y_res == 0].shape[0], y_res[y_res == 1].shape[0], y_res[y_res == 2].shape[0]\n",
    ")\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "xtrain = X_res\n",
    "ytrain = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Train and Generate models \n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakmodles = []\n",
    "estimators = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changelist(strlist):\n",
    "    test_list = (strlist).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\",\")\n",
    "    test_list = [int(i) for i in test_list]\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_12 = RandomForestClassifier(\n",
    "    random_state=randomseed,\n",
    "    n_estimators=100,\n",
    "    max_depth=12,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"auto\",\n",
    ")\n",
    "\n",
    "xgb_model_12 = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=randomseed)\n",
    "\n",
    "scv_12 = svmgpu(\n",
    "    C=100,\n",
    "    cache_size=None,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=None,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Rf_list)):\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=changelist(Rf_list.iloc[i, 0])), rf_12)\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_12\" + str(i),\n",
    "                (\n",
    "                    make_pipeline(\n",
    "                        ColumnSelector(cols=changelist(Rf_list.iloc[i, 0])), rf_12\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xgb_list)):\n",
    "    weakmodles.append(\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=changelist(xgb_list.iloc[i, 0])), xgb_model_12\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"xgb_model_12\" + str(i),\n",
    "                (\n",
    "                    make_pipeline(\n",
    "                        ColumnSelector(cols=changelist(xgb_list.iloc[i, 0])),\n",
    "                        xgb_model_12,\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(svm_list)):\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=changelist(svm_list.iloc[i, 0])), scv_12)\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"scv_12\" + str(i),\n",
    "                (\n",
    "                    make_pipeline(\n",
    "                        ColumnSelector(cols=changelist(svm_list.iloc[i, 0])), scv_12\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Train ensemble modles\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingclf2 = VotingClassifier(estimators=estimators[:25], voting=\"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingclf2.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = votingclf2.predict(xtest.astype(float))\n",
    "\n",
    "print(\"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "confmatrx = pd.DataFrame(\n",
    "    m.confusion_matrix(ytest, ypred),\n",
    "    columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    ")\n",
    "confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(\n",
    "    votingclf2, xtrain, ytrain, cv=10, scoring=\"accuracy\"\n",
    ")\n",
    "print(\n",
    "    \"Accuracy: %0.2f (+/- %0.2f) [%s] \\n [%s]\"\n",
    "    % (scores.mean(), scores.std(), \"SVC_clf\", scores)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
